{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d78b7d7",
   "metadata": {},
   "source": [
    "#### Quadrant Vec store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1253ffd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports loaded correctly!\n",
      "‚úì Using langchain_core.documents.Document (correct LangChain 1.0+ import)\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# LangChain core - Document class\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Qdrant imports\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, Filter, FieldCondition, MatchValue ## Metadata filtering and index creation.\n",
    "\n",
    "# Ollama embeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "print(\"‚úì All imports loaded correctly!\")\n",
    "print(\"‚úì Using langchain_core.documents.Document (correct LangChain 1.0+ import)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab25cabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                     ID              SIZE      MODIFIED    \n",
      "gemma3:1b                8648f39daa8f    815 MB    4 weeks ago    \n",
      "nomic-embed-text:v1.5    0a109f422b47    274 MB    4 weeks ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0391dfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Ollama embeddings (nomic-embed-text)...\n",
      "Make sure Ollama is running: 'ollama serve'\n",
      "\n",
      "‚úì Ollama embeddings initialized\n",
      "  Model: nomic-embed-text\n",
      "  Dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ollama embeddings\n",
    "# This connects to your local Ollama service and uses the nomic-embed-text model\n",
    "\n",
    "print(\"Initializing Ollama embeddings (nomic-embed-text)...\")\n",
    "print(\"Make sure Ollama is running: 'ollama serve'\\n\")\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text:v1.5\")\n",
    "\n",
    "print(\"‚úì Ollama embeddings initialized\")\n",
    "print(\"  Model: nomic-embed-text\")\n",
    "print(\"  Dimension: 768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5968dd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created 3 sample documents:\n",
      "  1. RAG combines retrieval and generation\n",
      "     Metadata: {'topic': 'rag', 'difficulty': 'intermediate'}\n",
      "  2. LangChain simplifies LLM applications\n",
      "     Metadata: {'topic': 'langchain', 'difficulty': 'beginner'}\n",
      "  3. Vector databases enable semantic search\n",
      "     Metadata: {'topic': 'vectordb', 'difficulty': 'intermediate'}\n"
     ]
    }
   ],
   "source": [
    "# Create sample documents with metadata\n",
    "# The Document class comes from langchain_core.documents\n",
    "\n",
    "sample_docs = [\n",
    "    Document(\n",
    "        page_content=\"RAG combines retrieval and generation\",\n",
    "        metadata={\"topic\": \"rag\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LangChain simplifies LLM applications\",\n",
    "        metadata={\"topic\": \"langchain\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Vector databases enable semantic search\",\n",
    "        metadata={\"topic\": \"vectordb\", \"difficulty\": \"intermediate\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"‚úì Created 3 sample documents:\")\n",
    "for i, doc in enumerate(sample_docs, 1):\n",
    "    print(f\"  {i}. {doc.page_content}\")\n",
    "    print(f\"     Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5911017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QDRANT IN-MEMORY EXAMPLE\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shara\\AppData\\Local\\Temp\\ipykernel_8808\\3668306292.py:14: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant_client_memory.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Added documents to Qdrant (in-memory)\n",
      "  Collection: my_collection_memory\n",
      "  Documents: 3\n",
      "  Storage: RAM (temporary)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"QDRANT IN-MEMORY EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Step 1: Create in-memory Qdrant client\n",
    "# The `:memory:` location means data is stored in RAM (not saved to disk)\n",
    "qdrant_client_memory = QdrantClient(location=\":memory:\")\n",
    "\n",
    "# Step 2: Create a collection\n",
    "# - collection_name: identifier for this collection\n",
    "# - size: must match embedding dimension (768 for nomic-embed-text)\n",
    "# - distance: COSINE measures similarity (other options: DOT, EUCLID)\n",
    "qdrant_client_memory.recreate_collection(\n",
    "    collection_name=\"my_collection_memory\",\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Step 3: Create Qdrant vector store wrapper\n",
    "# This LangChain wrapper makes it easy to work with Qdrant\n",
    "qdrant_store_memory = QdrantVectorStore(\n",
    "    client=qdrant_client_memory,\n",
    "    collection_name=\"my_collection_memory\",\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Step 4: Add documents to the store\n",
    "# This will automatically:\n",
    "# 1. Convert documents to embeddings using Ollama\n",
    "# 2. Store embeddings + metadata in Qdrant\n",
    "qdrant_store_memory.add_documents(sample_docs)\n",
    "print(\"‚úì Added documents to Qdrant (in-memory)\")\n",
    "print(\"  Collection: my_collection_memory\")\n",
    "print(\"  Documents: 3\")\n",
    "print(\"  Storage: RAM (temporary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df05cdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "BASIC SIMILARITY SEARCH\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: 'Tell me about RAG'\n",
      "\n",
      "Search results:\n",
      "  1. RAG combines retrieval and generation\n",
      "     Metadata: {'topic': 'rag', 'difficulty': 'intermediate', '_id': '06bad21381fe4110ab9e54e7c42325c1', '_collection_name': 'my_collection_memory'}\n",
      "  2. Vector databases enable semantic search\n",
      "     Metadata: {'topic': 'vectordb', 'difficulty': 'intermediate', '_id': '4e8db7b581a642708057722df73d8204', '_collection_name': 'my_collection_memory'}\n",
      "\n",
      "üí° Notice: The document about 'RAG combines retrieval...' is returned first\n",
      "   because it's semantically most similar to our query!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"BASIC SIMILARITY SEARCH\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Search for documents similar to this query\n",
    "# k=2 means return the top 2 most similar documents\n",
    "results = qdrant_store_memory.similarity_search(\n",
    "    \"Tell me about RAG\",\n",
    "    k=2\n",
    ")\n",
    "\n",
    "print(\"\\nQuery: 'Tell me about RAG'\")\n",
    "print(\"\\nSearch results:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"  {i}. {doc.page_content}\")\n",
    "    print(f\"     Metadata: {doc.metadata}\")\n",
    "\n",
    "print(\"\\nüí° Notice: The document about 'RAG combines retrieval...' is returned first\")\n",
    "print(\"   because it's semantically most similar to our query!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2d24a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'topic': 'rag', 'difficulty': 'intermediate', '_id': '06bad21381fe4110ab9e54e7c42325c1', '_collection_name': 'my_collection_memory'}, page_content='RAG combines retrieval and generation'),\n",
       " Document(metadata={'topic': 'vectordb', 'difficulty': 'intermediate', '_id': '4e8db7b581a642708057722df73d8204', '_collection_name': 'my_collection_memory'}, page_content='Vector databases enable semantic search')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a693baa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SEARCH WITH METADATA FILTER\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: 'Tell me about RAG'\n",
      "Filter: topic='rag'\n",
      "\n",
      "Filtered search results:\n",
      "  1. RAG combines retrieval and generation\n",
      "     Metadata: {'topic': 'rag', 'difficulty': 'intermediate', '_id': '06bad21381fe4110ab9e54e7c42325c1', '_collection_name': 'my_collection_memory'}\n",
      "\n",
      "üí° Only documents with topic='rag' are returned!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"SEARCH WITH METADATA FILTER\")\n",
    "print(\"-\" * 80)\n",
    "#metadata={\"topic\": [\"rag\", \"llms\", \"agents\"]}\n",
    "# Create a filter to only search documents with topic='rag'\n",
    "# Note: We use 'metadata.topic' because metadata is nested\n",
    "qdrant_filter = Filter(\n",
    "    must=[\n",
    "        FieldCondition(\n",
    "            key=\"metadata.topic\",\n",
    "            match=MatchValue(value=\"rag\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Same search, but only among filtered documents\n",
    "results_filtered = qdrant_store_memory.similarity_search(\n",
    "    \"Tell me about RAG\",\n",
    "    k=2,\n",
    "    filter=qdrant_filter\n",
    ")\n",
    "\n",
    "print(\"\\nQuery: 'Tell me about RAG'\")\n",
    "print(\"Filter: topic='rag'\")\n",
    "print(\"\\nFiltered search results:\")\n",
    "for i, doc in enumerate(results_filtered, 1):\n",
    "    print(f\"  {i}. {doc.page_content}\")\n",
    "    print(f\"     Metadata: {doc.metadata}\")\n",
    "\n",
    "print(\"\\nüí° Only documents with topic='rag' are returned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68ec60fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MULTIPLE FILTERS EXAMPLE (AND LOGIC)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Filter structure:\n",
      "  must=[\n",
      "    FieldCondition(key='metadata.topic', match='rag'),\n",
      "    FieldCondition(key='metadata.difficulty', match='intermediate')\n",
      "  ]\n",
      "\n",
      "üí° Both conditions must be true (AND logic)\n",
      "üí° For OR logic, use should=[...] instead of must=[...]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"MULTIPLE FILTERS EXAMPLE (AND LOGIC)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Filter for documents where:\n",
    "# - topic='rag' AND\n",
    "# - difficulty='intermediate'\n",
    "multi_filter = Filter(\n",
    "    must=[\n",
    "        FieldCondition(key=\"metadata.topic\", match=MatchValue(value=\"rag\")),\n",
    "        FieldCondition(key=\"metadata.difficulty\", match=MatchValue(value=\"intermediate\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# You can use this filter in similarity_search:\n",
    "# results_multi = qdrant_store_memory.similarity_search(\"RAG\", k=2, filter=multi_filter)\n",
    "\n",
    "print(\"\\nFilter structure:\")\n",
    "print(\"  must=[\")\n",
    "print(\"    FieldCondition(key='metadata.topic', match='rag'),\")\n",
    "print(\"    FieldCondition(key='metadata.difficulty', match='intermediate')\")\n",
    "print(\"  ]\")\n",
    "print(\"\\nüí° Both conditions must be true (AND logic)\")\n",
    "print(\"üí° For OR logic, use should=[...] instead of must=[...]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bc53072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'Tell me about RAG'\n",
      "Filter: topic='rag'\n",
      "\n",
      "Filtered search results:\n",
      "  1. RAG combines retrieval and generation\n",
      "     Metadata: {'topic': 'rag', 'difficulty': 'intermediate', '_id': '06bad21381fe4110ab9e54e7c42325c1', '_collection_name': 'my_collection_memory'}\n",
      "\n",
      "üí° Only documents with topic='rag' are returned!\n"
     ]
    }
   ],
   "source": [
    "# Same search, but only among filtered documents\n",
    "results_filtered = qdrant_store_memory.similarity_search(\n",
    "    \"Tell me about RAG\",\n",
    "    k=2,\n",
    "    filter=multi_filter\n",
    ")\n",
    "\n",
    "print(\"\\nQuery: 'Tell me about RAG'\")\n",
    "print(\"Filter: topic='rag'\")\n",
    "print(\"\\nFiltered search results:\")\n",
    "for i, doc in enumerate(results_filtered, 1):\n",
    "    print(f\"  {i}. {doc.page_content}\")\n",
    "    print(f\"     Metadata: {doc.metadata}\")\n",
    "\n",
    "print(\"\\nüí° Only documents with topic='rag' are returned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0ac7a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QDRANT WITH LOCAL PERSISTENCE\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shara\\AppData\\Local\\Temp\\ipykernel_8808\\1087051481.py:14: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant_client_persistent.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Added documents to Qdrant (persistent)\n",
      "  Storage location: ./qdrant_data\n",
      "  Collection: my_collection_persistent\n",
      "  ‚ö†Ô∏è  Data will persist even after this script ends!\n",
      "\n",
      "Query: 'Tell me about LangChain'\n",
      "\n",
      "Search results:\n",
      "  1. LangChain simplifies LLM applications\n",
      "     Metadata: {'topic': 'langchain', 'difficulty': 'beginner', '_id': '75c0a44296694e79aa09f56c8d4e891f', '_collection_name': 'my_collection_persistent'}\n",
      "  2. RAG combines retrieval and generation\n",
      "     Metadata: {'topic': 'rag', 'difficulty': 'intermediate', '_id': 'a6894c57eddc494889e351dd6b4ff3ef', '_collection_name': 'my_collection_persistent'}\n",
      "\n",
      "üí° Next time you run this, you can load the same data from disk!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QDRANT WITH LOCAL PERSISTENCE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Step 1: Specify a local directory for storage\n",
    "qdrant_path = \"./qdrant_data\"\n",
    "\n",
    "# Step 2: Create persistent Qdrant client\n",
    "# Data will be saved in the ./qdrant_data directory\n",
    "qdrant_client_persistent = QdrantClient(path=qdrant_path)\n",
    "\n",
    "# Step 3: Create collection (same as before)\n",
    "qdrant_client_persistent.recreate_collection(\n",
    "    collection_name=\"my_collection_persistent\",\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Step 4: Create vector store wrapper\n",
    "qdrant_store_persistent = QdrantVectorStore(\n",
    "    client=qdrant_client_persistent,\n",
    "    collection_name=\"my_collection_persistent\",\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Step 5: Add documents\n",
    "qdrant_store_persistent.add_documents(sample_docs)\n",
    "print(f\"‚úì Added documents to Qdrant (persistent)\")\n",
    "print(f\"  Storage location: {qdrant_path}\")\n",
    "print(f\"  Collection: my_collection_persistent\")\n",
    "print(f\"  ‚ö†Ô∏è  Data will persist even after this script ends!\")\n",
    "\n",
    "# Step 6: Search\n",
    "results = qdrant_store_persistent.similarity_search(\n",
    "    \"Tell me about LangChain\",\n",
    "    k=2\n",
    ")\n",
    "\n",
    "print(\"\\nQuery: 'Tell me about LangChain'\")\n",
    "print(\"\\nSearch results:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"  {i}. {doc.page_content}\")\n",
    "    print(f\"     Metadata: {doc.metadata}\")\n",
    "\n",
    "print(\"\\nüí° Next time you run this, you can load the same data from disk!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37acda82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QDRANT FROM_DOCUMENTS (RECOMMENDED METHOD)\n",
      "================================================================================\n",
      "\n",
      "‚úì Created Qdrant store from documents\n",
      "  Collection: rag_collection\n",
      "  Storage: ./qdrant_easy\n",
      "  Documents: 3\n",
      "\n",
      "üí° This is the recommended approach for most use cases!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QDRANT FROM_DOCUMENTS (RECOMMENDED METHOD)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Create Qdrant store directly from documents\n",
    "# This is the easiest way - everything happens in one call!\n",
    "qdrant_store_easy = QdrantVectorStore.from_documents(\n",
    "    documents=sample_docs,          # Your documents\n",
    "    embedding=embeddings,            # Embedding function\n",
    "    path=\"./qdrant_easy\",           # Local persistence (optional)\n",
    "    collection_name=\"rag_collection\" # Collection name\n",
    ")\n",
    "\n",
    "print(\"‚úì Created Qdrant store from documents\")\n",
    "print(\"  Collection: rag_collection\")\n",
    "print(\"  Storage: ./qdrant_easy\")\n",
    "print(\"  Documents: 3\")\n",
    "print(\"\\nüí° This is the recommended approach for most use cases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9455efff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'Vector databases'\n",
      "\n",
      "Search results with similarity scores:\n",
      "\n",
      "  Score: 0.7914\n",
      "  Content: Vector databases enable semantic search\n",
      "  Metadata: {'topic': 'vectordb', 'difficulty': 'intermediate', '_id': '274e4b892f134968bfbb4a41ba407572', '_collection_name': 'rag_collection'}\n",
      "\n",
      "  Score: 0.4856\n",
      "  Content: RAG combines retrieval and generation\n",
      "  Metadata: {'topic': 'rag', 'difficulty': 'intermediate', '_id': '73d2736a24ad477c97d6ed42370bdeab', '_collection_name': 'rag_collection'}\n",
      "\n",
      "  Score: 0.3989\n",
      "  Content: LangChain simplifies LLM applications\n",
      "  Metadata: {'topic': 'langchain', 'difficulty': 'beginner', '_id': '264d12527d634f748051b0a5b380cdac', '_collection_name': 'rag_collection'}\n",
      "\n",
      "üí° Scores help you filter out low-quality results\n",
      "üí° You can set a threshold (e.g., only return results with score > 0.7)\n"
     ]
    }
   ],
   "source": [
    "# Search with scores\n",
    "results_with_scores = qdrant_store_easy.similarity_search_with_score(\n",
    "    \"Vector databases\",\n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(\"\\nQuery: 'Vector databases'\")\n",
    "print(\"\\nSearch results with similarity scores:\")\n",
    "print()\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"  Score: {score:.4f}\")  # Similarity score (higher = more similar)\n",
    "    print(f\"  Content: {doc.page_content}\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")\n",
    "    print()\n",
    "\n",
    "print(\"üí° Scores help you filter out low-quality results\")\n",
    "print(\"üí° You can set a threshold (e.g., only return results with score > 0.7)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a90526",
   "metadata": {},
   "source": [
    "#### Weaviate Vec-store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d3c7f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "WEAVIATE LOCAL VECTOR STORE EXAMPLE\n",
      "================================================================================\n",
      "\n",
      "‚ö†Ô∏è  Note: This requires Weaviate running locally on port 8080\n",
      "   If not running, you'll see connection errors (that's OK for learning!)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Connecting to Local Weaviate\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Connected to local Weaviate\n",
      "  Host: localhost:8080\n",
      "  gRPC Port: 50051\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Creating Weaviate Vector Store\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Added documents to Weaviate\n",
      "  Index: MyDocuments\n",
      "  Documents: 3\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Basic Search\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: 'Tell me about RAG'\n",
      "\n",
      "Search results:\n",
      "  1. RAG combines retrieval and generation\n",
      "     Metadata: {'difficulty': 'intermediate', 'topic': 'rag'}\n",
      "  2. Vector databases enable semantic search\n",
      "     Metadata: {'difficulty': 'intermediate', 'topic': 'vectordb'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Search with Metadata Filter\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: 'Tell me about databases'\n",
      "Filter: difficulty='intermediate'\n",
      "\n",
      "Filtered search results:\n",
      "  1. Vector databases enable semantic search\n",
      "     Metadata: {'difficulty': 'intermediate', 'topic': 'vectordb'}\n",
      "  2. RAG combines retrieval and generation\n",
      "     Metadata: {'difficulty': 'intermediate', 'topic': 'rag'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Search with Scores\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: 'Vector databases'\n",
      "\n",
      "Search results with scores:\n",
      "  Score: 1.0000\n",
      "  Content: Vector databases enable semantic search\n",
      "  Metadata: {'difficulty': 'intermediate', 'topic': 'vectordb'}\n",
      "\n",
      "  Score: 0.1546\n",
      "  Content: RAG combines retrieval and generation\n",
      "  Metadata: {'difficulty': 'intermediate', 'topic': 'rag'}\n",
      "\n",
      "  Score: 0.0000\n",
      "  Content: LangChain simplifies LLM applications\n",
      "  Metadata: {'difficulty': 'beginner', 'topic': 'langchain'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Creating Weaviate from Documents (Alternative Method)\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Created Weaviate store from documents\n",
      "\n",
      "Quick search results:\n",
      "  1. LangChain simplifies LLM applications\n",
      "  2. Vector databases enable semantic search\n",
      "\n",
      "‚úì Closed Weaviate connection\n"
     ]
    }
   ],
   "source": [
    "from weaviate.classes.query import Filter\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"WEAVIATE LOCAL VECTOR STORE EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"‚ö†Ô∏è  Note: This requires Weaviate running locally on port 8080\")\n",
    "print(\"   If not running, you'll see connection errors (that's OK for learning!)\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    import weaviate\n",
    "    from langchain_weaviate import WeaviateVectorStore\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(\"Connecting to Local Weaviate\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Step 1: Connect to local Weaviate instance\n",
    "    # This assumes Weaviate is running on localhost:8080\n",
    "    weaviate_client = weaviate.connect_to_local(\n",
    "        host=\"localhost\",\n",
    "        port=8080,\n",
    "        grpc_port=50051\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Connected to local Weaviate\")\n",
    "    print(\"  Host: localhost:8080\")\n",
    "    print(\"  gRPC Port: 50051\")\n",
    "    \n",
    "    # Step 2: Create Weaviate vector store\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Creating Weaviate Vector Store\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    weaviate_store = WeaviateVectorStore(\n",
    "        client=weaviate_client,\n",
    "        index_name=\"MyDocuments\",  # Collection name in Weaviate\n",
    "        text_key=\"text\",            # Field name for document text\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    \n",
    "    # Step 3: Add documents\n",
    "    weaviate_store.add_documents(sample_docs)\n",
    "    print(\"‚úì Added documents to Weaviate\")\n",
    "    print(\"  Index: MyDocuments\")\n",
    "    print(\"  Documents: 3\")\n",
    "    \n",
    "    # Step 4: Basic Search\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Basic Search\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    results = weaviate_store.similarity_search(\n",
    "        \"Tell me about RAG\",\n",
    "        k=2\n",
    "    )\n",
    "    \n",
    "    print(\"\\nQuery: 'Tell me about RAG'\")\n",
    "    print(\"\\nSearch results:\")\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"  {i}. {doc.page_content}\")\n",
    "        print(f\"     Metadata: {doc.metadata}\")\n",
    "    \n",
    "    # Step 5: Search with Metadata Filter\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Search with Metadata Filter\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Weaviate uses where_filter with different syntax\n",
    "    results_filtered = weaviate_store.similarity_search(\"Tell me about databases\", \n",
    "    k=2, \n",
    "    filters=Filter.by_property(\"difficulty\").equal(\"intermediate\") ) # Proper Filter object\n",
    "    \n",
    "    print(\"\\nQuery: 'Tell me about databases'\")\n",
    "    print(\"Filter: difficulty='intermediate'\")\n",
    "    print(\"\\nFiltered search results:\")\n",
    "    for i, doc in enumerate(results_filtered, 1):\n",
    "        print(f\"  {i}. {doc.page_content}\")\n",
    "        print(f\"     Metadata: {doc.metadata}\")\n",
    "    \n",
    "    # Step 6: Search with Scores\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Search with Scores\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    results_with_scores = weaviate_store.similarity_search_with_score(\n",
    "        \"Vector databases\",\n",
    "        k=3\n",
    "    )\n",
    "    \n",
    "    print(\"\\nQuery: 'Vector databases'\")\n",
    "    print(\"\\nSearch results with scores:\")\n",
    "    for doc, score in results_with_scores:\n",
    "        print(f\"  Score: {score:.4f}\")\n",
    "        print(f\"  Content: {doc.page_content}\")\n",
    "        print(f\"  Metadata: {doc.metadata}\")\n",
    "        print()\n",
    "    \n",
    "    # Step 7: Alternative - Create from Documents\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Creating Weaviate from Documents (Alternative Method)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    weaviate_store_easy = WeaviateVectorStore.from_documents(\n",
    "        documents=sample_docs,\n",
    "        embedding=embeddings,\n",
    "        client=weaviate_client,\n",
    "        index_name=\"EasyDocuments\"\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Created Weaviate store from documents\")\n",
    "    \n",
    "    # Quick search\n",
    "    results = weaviate_store_easy.similarity_search(\"LangChain\", k=2)\n",
    "    print(\"\\nQuick search results:\")\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"  {i}. {doc.page_content}\")\n",
    "    \n",
    "    # Clean up\n",
    "    weaviate_client.close()\n",
    "    print(\"\\n‚úì Closed Weaviate connection\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Weaviate error: {e}\")\n",
    "    print()\n",
    "    print(\"Troubleshooting:\")\n",
    "    print(\"1. Check if Weaviate is running: docker ps\")\n",
    "    print(\"2. Start Weaviate: docker run -d -p 8080:8080 -p 50051:50051 \\\\\")\n",
    "    print(\"     --name weaviate cr.weaviate.io/semitechnologies/weaviate:latest\")\n",
    "    print(\"3. Check if port 8080 is available: lsof -i :8080\")\n",
    "    print(\"4. Check Weaviate logs: docker logs weaviate\")\n",
    "    print()\n",
    "    print(\"üí° It's OK if this doesn't work - you can still learn from the code!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bdf2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
