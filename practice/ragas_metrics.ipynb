{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd73ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "484ebd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GOOGLE_API_KEY found\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "# Verify API key\n",
    "if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    print(\"âœ… GOOGLE_API_KEY found\")\n",
    "else:\n",
    "    print(\"âŒ GOOGLE_API_KEY not found - please set it in your .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a82a50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shara\\OneDrive\\Documents\\Live Courses Krish Naik\\R_A_G\\Simple_rag\\langchain\\Lib\\site-packages\\instructor\\providers\\gemini\\client.py:6: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Core Imports\n",
    "\n",
    "# Standard library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "# LangChain components\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# RAGAS components\n",
    "from ragas import SingleTurnSample, EvaluationDataset, evaluate\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    ResponseRelevancy,\n",
    "    LLMContextPrecisionWithReference,\n",
    "    LLMContextRecall,\n",
    "    ContextEntityRecall,\n",
    "    NoiseSensitivity\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "print(\"âœ… All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9138243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                     ID              SIZE      MODIFIED    \n",
      "gemma3:1b                8648f39daa8f    815 MB    5 weeks ago    \n",
      "nomic-embed-text:v1.5    0a109f422b47    274 MB    5 weeks ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e46b077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings, ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db79ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"gemma3:1b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ec5bf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM initialized: gpt-4o-mini\n",
      "âœ… Embeddings initialized: text-embedding-3-small\n",
      "âœ… RAGAS wrappers ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shara\\AppData\\Local\\Temp\\ipykernel_22344\\1223316257.py:18: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  ragas_llm = LangchainLLMWrapper(llm)\n",
      "C:\\Users\\shara\\AppData\\Local\\Temp\\ipykernel_22344\\1223316257.py:19: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM and Embeddings\n",
    "\n",
    "# Initialize base models  (OPENAI)\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text:v1.5\",\n",
    "    # base_url=\"http://localhost:11434\"  # Default Ollama URL\n",
    ")\n",
    "\n",
    "\n",
    "#  embeddings= GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "# Initialize base models (GEMINI)\n",
    "#llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "#embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "# Wrap for RAGAS compatibility\n",
    "ragas_llm = LangchainLLMWrapper(llm)\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "print(\"âœ… LLM initialized: gpt-4o-mini\")\n",
    "print(\"âœ… Embeddings initialized: text-embedding-3-small\")\n",
    "print(\"âœ… RAGAS wrappers ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "579fa405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Async helper ready\n"
     ]
    }
   ],
   "source": [
    "# Helper function for running async code in Jupyter\n",
    "\n",
    "def run_async(coro):\n",
    "    \"\"\"Helper to run async code in Jupyter notebooks\"\"\"\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            # We're in Jupyter with an existing loop\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            return loop.run_until_complete(coro)\n",
    "        else:\n",
    "            return asyncio.run(coro)\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "\n",
    "print(\"âœ… Async helper ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a9850fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Response to evaluate:\n",
      "   'The first Super Bowl was held on January 15, 1967 in Los Angeles. It was a sunny day with clear skies.'\n",
      "\n",
      "ğŸ“š Retrieved context:\n",
      "   'The First AFL-NFL World Championship Game was played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles, California.'\n"
     ]
    }
   ],
   "source": [
    "# Define our test case\n",
    "\n",
    "# The response we want to evaluate\n",
    "test_response = \"The first Super Bowl was held on January 15, 1967 in Los Angeles. It was a sunny day with clear skies.\"\n",
    "\n",
    "# The context that was retrieved (source of truth)\n",
    "test_context = [\n",
    "    \"The First AFL-NFL World Championship Game was played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles, California.\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ Response to evaluate:\")\n",
    "print(f\"   '{test_response}'\")\n",
    "print(\"\\nğŸ“š Retrieved context:\")\n",
    "print(f\"   '{test_context[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7916e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63ede888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” STEP 1: Extracted Claims from Response\n",
      "==================================================\n",
      "Hereâ€™s a numbered list of the factual claims extracted from the response:\n",
      "\n",
      "1.  The first Super Bowl was held on January 15, 1967.\n",
      "2.  It was a sunny day.\n",
      "3.  It was a clear day. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manual claim extraction using LLM (mimicking RAGAS)\n",
    "\n",
    "claim_extraction_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the following response, extract ALL factual claims as a numbered list.\n",
    "Each claim should be a single, verifiable statement.\n",
    "\n",
    "Response: {response}\n",
    "\n",
    "Extract each factual claim:\n",
    "\"\"\")\n",
    "\n",
    "claim_chain = claim_extraction_prompt | llm | StrOutputParser()\n",
    "\n",
    "extracted_claims_raw = claim_chain.invoke({\"response\": test_response})\n",
    "\n",
    "print(\"ğŸ” STEP 1: Extracted Claims from Response\")\n",
    "print(\"=\" * 50)\n",
    "print(extracted_claims_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f33f0d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Claims to verify:\n",
      "   1. The first Super Bowl was held on January 15, 1967\n",
      "   2. The first Super Bowl was held in Los Angeles\n",
      "   3. It was a sunny day\n",
      "   4. There were clear skies\n"
     ]
    }
   ],
   "source": [
    "# Parse the claims into a list for verification\n",
    "\n",
    "# For our analysis, let's define the claims explicitly\n",
    "claims = [\n",
    "    \"The first Super Bowl was held on January 15, 1967\",\n",
    "    \"The first Super Bowl was held in Los Angeles\",\n",
    "    \"It was a sunny day\",\n",
    "    \"There were clear skies\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“‹ Claims to verify:\")\n",
    "for i, claim in enumerate(claims,1):\n",
    "    print(f\"   {i}. {claim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8962157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” STEP 2: Verifying Each Claim Against Context\n",
      "============================================================\n",
      "\n",
      "âŒ Claim: 'The first Super Bowl was held on January 15, 1967'\n",
      "   Result: Verdict: NOT SUPPORTED\n",
      "\n",
      "Explanation: The context explicitly states the AFL-NFL World Championship Ga...\n",
      "\n",
      "âŒ Claim: 'The first Super Bowl was held in Los Angeles'\n",
      "   Result: Verdict: NOT SUPPORTED\n",
      "\n",
      "Explanation: The context explicitly states the AFL-NFL World Championship Ga...\n",
      "\n",
      "âŒ Claim: 'It was a sunny day'\n",
      "   Result: Verdict: NOT SUPPORTED\n",
      "\n",
      "Explanation: The context states the game was played on a specific date and l...\n",
      "\n",
      "âŒ Claim: 'There were clear skies'\n",
      "   Result: Verdict: NOT SUPPORTED\n",
      "\n",
      "Explanation: The context states the game was played on January 15, 1967, at ...\n"
     ]
    }
   ],
   "source": [
    "# Manual claim verification (mimicking RAGAS)\n",
    "\n",
    "verification_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the following context and claim, determine if the claim is SUPPORTED by the context.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Claim: {claim}\n",
    "\n",
    "Answer with:\n",
    "- \"SUPPORTED\" if the claim can be verified from the context\n",
    "- \"NOT SUPPORTED\" if the claim cannot be verified or contradicts the context\n",
    "\n",
    "Also provide a brief explanation.\n",
    "\n",
    "Verdict:\n",
    "\"\"\")\n",
    "\n",
    "verify_chain = verification_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"ğŸ” STEP 2: Verifying Each Claim Against Context\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "verification_results = []\n",
    "for claim in claims:\n",
    "    result = verify_chain.invoke({\n",
    "        \"context\": test_context[0],\n",
    "        \"claim\": claim\n",
    "    })\n",
    "    is_supported = \"SUPPORTED\" in result.upper() and \"NOT SUPPORTED\" not in result.upper()\n",
    "    verification_results.append({\n",
    "        \"claim\": claim,\n",
    "        \"supported\": is_supported,\n",
    "        \"explanation\": result\n",
    "    })\n",
    "    status = \"âœ…\" if is_supported else \"âŒ\"\n",
    "    print(f\"\\n{status} Claim: '{claim}'\")\n",
    "    print(f\"   Result: {result[:100]}...\" if len(result) > 100 else f\"   Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9193e6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Claim Verification Summary\n",
      "================================================================================\n",
      "                                            Claim Supported?                          Reason\n",
      "The first Super Bowl was held on January 15, 1967       âŒ No HALLUCINATION - Not in context!\n",
      "     The first Super Bowl was held in Los Angeles       âŒ No HALLUCINATION - Not in context!\n",
      "                               It was a sunny day       âŒ No HALLUCINATION - Not in context!\n",
      "                           There were clear skies       âŒ No HALLUCINATION - Not in context!\n"
     ]
    }
   ],
   "source": [
    "# Display verification results as a table\n",
    "\n",
    "print(\"\\nğŸ“Š Claim Verification Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_verification = pd.DataFrame([\n",
    "    {\n",
    "        \"Claim\": r[\"claim\"],\n",
    "        \"Supported?\": \"âœ… Yes\" if r[\"supported\"] else \"âŒ No\",\n",
    "        \"Reason\": \"Found in context\" if r[\"supported\"] else \"HALLUCINATION - Not in context!\"\n",
    "    }\n",
    "    for r in verification_results\n",
    "])\n",
    "\n",
    "print(df_verification.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a69f9d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ STEP 3: Calculate Faithfulness Score\n",
      "==================================================\n",
      "\n",
      "   Supported claims: 0\n",
      "   Total claims: 4\n",
      "\n",
      "   Formula: Faithfulness = 0 / 4\n",
      "\n",
      "   ğŸ“Š Manual Faithfulness Score: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Manual faithfulness calculation\n",
    "\n",
    "supported_count = sum(1 for r in verification_results if r[\"supported\"])\n",
    "total_claims = len(verification_results)\n",
    "\n",
    "manual_faithfulness = supported_count / total_claims\n",
    "\n",
    "print(\"ğŸ”¢ STEP 3: Calculate Faithfulness Score\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Supported claims: {supported_count}\")\n",
    "print(f\"   Total claims: {total_claims}\")\n",
    "print(f\"\\n   Formula: Faithfulness = {supported_count} / {total_claims}\")\n",
    "print(f\"\\n   ğŸ“Š Manual Faithfulness Score: {manual_faithfulness:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efb5a86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ RAGAS Faithfulness Result\n",
      "==================================================\n",
      "\n",
      "   Manual calculation:  0.00\n",
      "   RAGAS metric score:  0.75\n",
      "\n",
      "   Difference: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Run actual RAGAS Faithfulness metric\n",
    "\n",
    "# Create sample in RAGAS format\n",
    "faithfulness_sample = SingleTurnSample(\n",
    "    user_input=\"When was the first Super Bowl?\",\n",
    "    response=test_response,\n",
    "    retrieved_contexts=test_context\n",
    ")\n",
    "\n",
    "# Initialize and run the metric\n",
    "faithfulness_metric = Faithfulness(llm=ragas_llm)\n",
    "\n",
    "ragas_faithfulness = run_async(faithfulness_metric.single_turn_ascore(faithfulness_sample))\n",
    "\n",
    "print(\"ğŸ”¬ RAGAS Faithfulness Result\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Manual calculation:  {manual_faithfulness:.2f}\")\n",
    "print(f\"   RAGAS metric score:  {ragas_faithfulness:.2f}\")\n",
    "print(f\"\\n   Difference: {abs(manual_faithfulness - ragas_faithfulness):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f687f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Faithfulness Comparison: Different Scenarios\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ·ï¸  Perfect Faithfulness (No hallucinations)\n",
      "   Response: 'The first Super Bowl was played on January 15, 1967 at the Los Angeles Memorial ...'\n",
      "   Score: 1.0000\n",
      "\n",
      "ğŸ·ï¸  Partial Faithfulness (Some hallucinations)\n",
      "   Response: 'The first Super Bowl was on January 15, 1967. The Green Bay Packers won 35-10 wi...'\n",
      "   Score: 1.0000\n",
      "\n",
      "ğŸ·ï¸  Zero Faithfulness (Complete hallucination)\n",
      "   Response: 'The first Super Bowl was held in Miami in 1970 and attracted over 100,000 specta...'\n",
      "   Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Compare different faithfulness scenarios\n",
    "\n",
    "faithfulness_examples = [\n",
    "    {\n",
    "        \"name\": \"Perfect Faithfulness (No hallucinations)\",\n",
    "        \"response\": \"The first Super Bowl was played on January 15, 1967 at the Los Angeles Memorial Coliseum.\",\n",
    "        \"context\": [\"The First AFL-NFL World Championship Game was played on January 15, 1967, at the Los Angeles Memorial Coliseum.\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Partial Faithfulness (Some hallucinations)\",\n",
    "        \"response\": \"The first Super Bowl was on January 15, 1967. The Green Bay Packers won 35-10 with Bart Starr as MVP.\",\n",
    "        \"context\": [\"The First AFL-NFL World Championship Game was played on January 15, 1967.\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Zero Faithfulness (Complete hallucination)\",\n",
    "        \"response\": \"The first Super Bowl was held in Miami in 1970 and attracted over 100,000 spectators.\",\n",
    "        \"context\": [\"The First AFL-NFL World Championship Game was played on January 15, 1967, at the Los Angeles Memorial Coliseum.\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ğŸ“Š Faithfulness Comparison: Different Scenarios\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for example in faithfulness_examples:\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=\"Tell me about the first Super Bowl\",\n",
    "        response=example[\"response\"],\n",
    "        retrieved_contexts=example[\"context\"]\n",
    "    )\n",
    "    score = run_async(faithfulness_metric.single_turn_ascore(sample))\n",
    "    \n",
    "    print(f\"\\nğŸ·ï¸  {example['name']}\")\n",
    "    print(f\"   Response: '{example['response'][:80]}...'\" if len(example['response']) > 80 else f\"   Response: '{example['response']}'\")\n",
    "    print(f\"   Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfbe6657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Original Question:\n",
      "   'When was the first Super Bowl?'\n",
      "\n",
      "ğŸ“ Answer to Evaluate:\n",
      "   'The first Super Bowl was held on January 15, 1967'\n"
     ]
    }
   ],
   "source": [
    "# Define our test case for relevancy\n",
    "\n",
    "original_question = \"When was the first Super Bowl?\"\n",
    "test_answer = \"The first Super Bowl was held on January 15, 1967\"\n",
    "\n",
    "print(\"ğŸ“ Original Question:\")\n",
    "print(f\"   '{original_question}'\")\n",
    "print(\"\\nğŸ“ Answer to Evaluate:\")\n",
    "print(f\"   '{test_answer}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00a107bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” STEP 1: Generated Hypothetical Questions\n",
      "==================================================\n",
      "Here are 3 questions generated from the provided answer, suitable for a variety of levels of engagement:\n",
      "\n",
      "1.  What year did the first Super Bowl take place?\n",
      "2.  Besides being a sporting event, what significant event occurred on January 15, 1967?\n",
      "3.  Can you name the year the Super Bowl was established?\n"
     ]
    }
   ],
   "source": [
    "# Manual hypothetical question generation (mimicking RAGAS)\n",
    "\n",
    "question_gen_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the following answer, generate exactly 3 different questions that this answer would be a good response to.\n",
    "The questions should be varied but all answerable by this response.\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Generate 3 questions (one per line):\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "\"\"\")\n",
    "\n",
    "question_gen_chain = question_gen_prompt | llm | StrOutputParser()\n",
    "\n",
    "generated_questions_raw = question_gen_chain.invoke({\"answer\": test_answer})\n",
    "\n",
    "print(\"ğŸ” STEP 1: Generated Hypothetical Questions\")\n",
    "print(\"=\" * 50)\n",
    "print(generated_questions_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bea18fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Questions for embedding comparison:\n",
      "   Original: 'When was the first Super Bowl?'\n",
      "   Generated:\n",
      "      1. 'When was the first Super Bowl held?'\n",
      "      2. 'What date was the inaugural Super Bowl?'\n",
      "      3. 'On what day did the first Super Bowl take place?'\n"
     ]
    }
   ],
   "source": [
    "# Parse generated questions (for our calculation)\n",
    "\n",
    "# Manually define likely generated questions\n",
    "generated_questions = [\n",
    "    \"When was the first Super Bowl held?\",\n",
    "    \"What date was the inaugural Super Bowl?\",\n",
    "    \"On what day did the first Super Bowl take place?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“‹ Questions for embedding comparison:\")\n",
    "print(f\"   Original: '{original_question}'\")\n",
    "print(\"   Generated:\")\n",
    "for i, q in enumerate(generated_questions, 1):\n",
    "    print(f\"      {i}. '{q}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b28a29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cosine similarity function ready\n",
      "\n",
      "ğŸ“ Formula: cos(Î¸) = (A Â· B) / (||A|| Ã— ||B||)\n"
     ]
    }
   ],
   "source": [
    "# Define cosine similarity function\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "print(\"âœ… Cosine similarity function ready\")\n",
    "print(\"\\nğŸ“ Formula: cos(Î¸) = (A Â· B) / (||A|| Ã— ||B||)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c37d35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” STEP 2: Computing Embeddings and Similarities\n",
      "============================================================\n",
      "\n",
      "âœ… Original question embedded (dim=768)\n",
      "\n",
      "   Question 1: 'When was the first Super Bowl held?'\n",
      "   Similarity to original: 0.9588\n",
      "\n",
      "   Question 2: 'What date was the inaugural Super Bowl?'\n",
      "   Similarity to original: 0.9097\n",
      "\n",
      "   Question 3: 'On what day did the first Super Bowl take place?'\n",
      "   Similarity to original: 0.9229\n"
     ]
    }
   ],
   "source": [
    "# Calculate embeddings and similarities\n",
    "\n",
    "print(\"ğŸ” STEP 2: Computing Embeddings and Similarities\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get embedding for original question\n",
    "original_embedding = embeddings.embed_query(original_question)\n",
    "print(f\"\\nâœ… Original question embedded (dim={len(original_embedding)})\")\n",
    "\n",
    "# Get embeddings for generated questions and calculate similarities\n",
    "similarities = []\n",
    "for i, gen_q in enumerate(generated_questions, 1):\n",
    "    gen_embedding = embeddings.embed_query(gen_q)\n",
    "    sim = cosine_similarity(original_embedding, gen_embedding)\n",
    "    similarities.append(sim)\n",
    "    print(f\"\\n   Question {i}: '{gen_q}'\")\n",
    "    print(f\"   Similarity to original: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cdc7ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ STEP 3: Calculate Answer Relevancy Score\n",
      "==================================================\n",
      "\n",
      "   Similarities: ['0.9588', '0.9097', '0.9229']\n",
      "   Formula: Average of similarities\n",
      "\n",
      "   (0.9588 + 0.9097 + 0.9229) / 3\n",
      "\n",
      "   ğŸ“Š Manual Answer Relevancy: 0.9305\n"
     ]
    }
   ],
   "source": [
    "# Calculate answer relevancy score\n",
    "\n",
    "manual_relevancy = np.mean(similarities)\n",
    "\n",
    "print(\"ğŸ”¢ STEP 3: Calculate Answer Relevancy Score\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Similarities: {[f'{s:.4f}' for s in similarities]}\")\n",
    "print(f\"   Formula: Average of similarities\")\n",
    "print(f\"\\n   ({' + '.join([f'{s:.4f}' for s in similarities])}) / {len(similarities)}\")\n",
    "print(f\"\\n   ğŸ“Š Manual Answer Relevancy: {manual_relevancy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46001b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ RAGAS Answer Relevancy Result\n",
      "==================================================\n",
      "\n",
      "   Manual calculation:  0.9305\n",
      "   RAGAS metric score:  0.9627\n"
     ]
    }
   ],
   "source": [
    "# Run actual RAGAS Answer Relevancy metric\n",
    "\n",
    "relevancy_sample = SingleTurnSample(\n",
    "    user_input=original_question,\n",
    "    response=test_answer,\n",
    "    retrieved_contexts=[\"The First AFL-NFL World Championship Game was played on January 15, 1967.\"]\n",
    ")\n",
    "\n",
    "relevancy_metric = ResponseRelevancy(llm=ragas_llm, embeddings=ragas_embeddings)\n",
    "\n",
    "ragas_relevancy = run_async(relevancy_metric.single_turn_ascore(relevancy_sample))\n",
    "\n",
    "print(\"ğŸ”¬ RAGAS Answer Relevancy Result\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Manual calculation:  {manual_relevancy:.4f}\")\n",
    "print(f\"   RAGAS metric score:  {ragas_relevancy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fdf0b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Answer Relevancy Comparison\n",
      "======================================================================\n",
      "\n",
      "ğŸ·ï¸  Highly Relevant (Directly answers WHEN)\n",
      "   Q: 'When was the first Super Bowl?'\n",
      "   A: 'The first Super Bowl was held on January 15, 1967.'\n",
      "   Score: 0.9632\n",
      "\n",
      "ğŸ·ï¸  Partially Relevant (Answers but adds extra info)\n",
      "   Q: 'When was the first Super Bowl?'\n",
      "   A: 'The Super Bowl is the annual championship game of the NFL, f...'\n",
      "   Score: 0.7506\n",
      "\n",
      "ğŸ·ï¸  Low Relevancy (Doesn't answer WHEN)\n",
      "   Q: 'When was the first Super Bowl?'\n",
      "   A: 'The Super Bowl is the annual championship game of the Nation...'\n",
      "   Score: 0.7506\n",
      "\n",
      "ğŸ·ï¸  Off-topic (Completely irrelevant)\n",
      "   Q: 'When was the first Super Bowl?'\n",
      "   A: 'Pizza is a popular Italian dish that spread worldwide in the...'\n",
      "   Score: 0.3138\n"
     ]
    }
   ],
   "source": [
    "# Compare different relevancy scenarios\n",
    "\n",
    "relevancy_examples = [\n",
    "    {\n",
    "        \"name\": \"Highly Relevant (Directly answers WHEN)\",\n",
    "        \"question\": \"When was the first Super Bowl?\",\n",
    "        \"answer\": \"The first Super Bowl was held on January 15, 1967.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Partially Relevant (Answers but adds extra info)\",\n",
    "        \"question\": \"When was the first Super Bowl?\",\n",
    "        \"answer\": \"The Super Bowl is the annual championship game of the NFL, first held on January 15, 1967.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Low Relevancy (Doesn't answer WHEN)\",\n",
    "        \"question\": \"When was the first Super Bowl?\",\n",
    "        \"answer\": \"The Super Bowl is the annual championship game of the National Football League.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Off-topic (Completely irrelevant)\",\n",
    "        \"question\": \"When was the first Super Bowl?\",\n",
    "        \"answer\": \"Pizza is a popular Italian dish that spread worldwide in the 20th century.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ğŸ“Š Answer Relevancy Comparison\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for example in relevancy_examples:\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=example[\"question\"],\n",
    "        response=example[\"answer\"],\n",
    "        retrieved_contexts=[\"Context not relevant for this metric.\"]\n",
    "    )\n",
    "    score = run_async(relevancy_metric.single_turn_ascore(sample))\n",
    "    \n",
    "    print(f\"\\nğŸ·ï¸  {example['name']}\")\n",
    "    print(f\"   Q: '{example['question']}'\")\n",
    "    print(f\"   A: '{example['answer'][:60]}...'\" if len(example['answer']) > 60 else f\"   A: '{example['answer']}'\")\n",
    "    print(f\"   Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14fc9de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Question: 'Where is the Eiffel Tower located?'\n",
      "\n",
      "ğŸ“š Retrieved Chunks (with relevance):\n",
      "   1. âœ… Relevant: 'The Eiffel Tower is located in Paris, France.'\n",
      "   2. âœ… Relevant: 'Paris is the capital of France.'\n",
      "   3. âŒ Not relevant: 'The tower was built in 1889.'\n",
      "   4. âŒ Not relevant: 'Pizza originated in Italy.'\n"
     ]
    }
   ],
   "source": [
    "# Define our test case for context precision\n",
    "\n",
    "question = \"Where is the Eiffel Tower located?\"\n",
    "reference = \"The Eiffel Tower is located in Paris, France.\"\n",
    "\n",
    "# Chunks with known relevance\n",
    "chunks_with_relevance = [\n",
    "    (\"The Eiffel Tower is located in Paris, France.\", True),      # Directly relevant\n",
    "    (\"Paris is the capital of France.\", True),                     # Somewhat relevant\n",
    "    (\"The tower was built in 1889.\", False),                       # Not relevant to WHERE\n",
    "    (\"Pizza originated in Italy.\", False),                         # Completely irrelevant\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ Question: '{}'\\n\".format(question))\n",
    "print(\"ğŸ“š Retrieved Chunks (with relevance):\")\n",
    "for i, (chunk, relevant) in enumerate(chunks_with_relevance, 1):\n",
    "    status = \"âœ… Relevant\" if relevant else \"âŒ Not relevant\"\n",
    "    print(f\"   {i}. {status}: '{chunk}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef0a7338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Manual Relevance Classification\n",
      "============================================================\n",
      "âœ… 'The Eiffel Tower is located in Paris, France....' â†’ RELEVANT\n",
      "âŒ 'Paris is the capital of France....' â†’ NOT RELEVANT\n",
      "âŒ 'The tower was built in 1889....' â†’ NOT RELEVANT\n",
      "âŒ 'Pizza originated in Italy....' â†’ NOT RELEVANT\n"
     ]
    }
   ],
   "source": [
    "# Manual relevance classification using LLM\n",
    "\n",
    "relevance_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the question and reference answer, determine if the following context chunk is RELEVANT.\n",
    "\n",
    "Question: {question}\n",
    "Reference Answer: {reference}\n",
    "Context Chunk: {chunk}\n",
    "\n",
    "Is this chunk relevant for answering the question? Answer only \"RELEVANT\" or \"NOT RELEVANT\".\n",
    "\"\"\")\n",
    "\n",
    "relevance_chain = relevance_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"ğŸ” Manual Relevance Classification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "relevance_results = []\n",
    "for chunk, expected in chunks_with_relevance:\n",
    "    result = relevance_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"reference\": reference,\n",
    "        \"chunk\": chunk\n",
    "    })\n",
    "    is_relevant = \"RELEVANT\" in result.upper() and \"NOT RELEVANT\" not in result.upper()\n",
    "    relevance_results.append(is_relevant)\n",
    "    status = \"âœ…\" if is_relevant else \"âŒ\"\n",
    "    print(f\"{status} '{chunk[:50]}...' â†’ {result.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "597eca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š GOOD RANKING: Relevant chunks at TOP\n",
      "============================================================\n",
      "\n",
      "Ranking: [âœ… Relevant, âœ… Relevant, âŒ Not Rel, âŒ Not Rel]\n",
      "\n",
      "Precision@K calculation:\n",
      "   Position 1: Precision@1 = 1/1 = 1.00 â†’ Contributes\n",
      "   Position 2: Precision@2 = 2/2 = 1.00 â†’ Contributes\n",
      "   Position 3: Precision@3 = 2/3 = 0.67 â†’ Does NOT contribute\n",
      "   Position 4: Precision@4 = 2/4 = 0.50 â†’ Does NOT contribute\n",
      "\n",
      "   Sum of contributing precisions: 2.00\n",
      "   Total relevant items: 2\n",
      "\n",
      "   ğŸ“Š Context Precision (Good Ranking): 1.00\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision@K for GOOD ranking (relevant at top)\n",
    "\n",
    "# Good ranking: [Relevant, Relevant, Not Relevant, Not Relevant]\n",
    "good_ranking = [True, True, False, False]\n",
    "\n",
    "print(\"ğŸ“Š GOOD RANKING: Relevant chunks at TOP\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nRanking: [âœ… Relevant, âœ… Relevant, âŒ Not Rel, âŒ Not Rel]\")\n",
    "print(\"\\nPrecision@K calculation:\")\n",
    "\n",
    "precisions_good = []\n",
    "relevant_count = 0\n",
    "for k, is_relevant in enumerate(good_ranking, 1):\n",
    "    if is_relevant:\n",
    "        relevant_count += 1\n",
    "    precision_at_k = relevant_count / k\n",
    "    contributes = \"â†’ Contributes\" if is_relevant else \"â†’ Does NOT contribute\"\n",
    "    print(f\"   Position {k}: Precision@{k} = {relevant_count}/{k} = {precision_at_k:.2f} {contributes}\")\n",
    "    if is_relevant:\n",
    "        precisions_good.append(precision_at_k)\n",
    "\n",
    "total_relevant = sum(good_ranking)\n",
    "context_precision_good = sum(precisions_good) / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "print(f\"\\n   Sum of contributing precisions: {sum(precisions_good):.2f}\")\n",
    "print(f\"   Total relevant items: {total_relevant}\")\n",
    "print(f\"\\n   ğŸ“Š Context Precision (Good Ranking): {context_precision_good:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d84b6269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š BAD RANKING: Relevant chunks at BOTTOM\n",
      "============================================================\n",
      "\n",
      "Ranking: [âŒ Not Rel, âŒ Not Rel, âœ… Relevant, âœ… Relevant]\n",
      "\n",
      "Precision@K calculation:\n",
      "   Position 1: Precision@1 = 0/1 = 0.00 â†’ Does NOT contribute\n",
      "   Position 2: Precision@2 = 0/2 = 0.00 â†’ Does NOT contribute\n",
      "   Position 3: Precision@3 = 1/3 = 0.33 â†’ Contributes\n",
      "   Position 4: Precision@4 = 2/4 = 0.50 â†’ Contributes\n",
      "\n",
      "   Sum of contributing precisions: 0.83\n",
      "   Total relevant items: 2\n",
      "\n",
      "   ğŸ“Š Context Precision (Bad Ranking): 0.42\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision@K for BAD ranking (relevant at bottom)\n",
    "\n",
    "# Bad ranking: [Not Relevant, Not Relevant, Relevant, Relevant]\n",
    "bad_ranking = [False, False, True, True]\n",
    "\n",
    "print(\"ğŸ“Š BAD RANKING: Relevant chunks at BOTTOM\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nRanking: [âŒ Not Rel, âŒ Not Rel, âœ… Relevant, âœ… Relevant]\")\n",
    "print(\"\\nPrecision@K calculation:\")\n",
    "\n",
    "precisions_bad = []\n",
    "relevant_count = 0\n",
    "for k, is_relevant in enumerate(bad_ranking, 1):\n",
    "    if is_relevant:\n",
    "        relevant_count += 1\n",
    "    precision_at_k = relevant_count / k\n",
    "    contributes = \"â†’ Contributes\" if is_relevant else \"â†’ Does NOT contribute\"\n",
    "    print(f\"   Position {k}: Precision@{k} = {relevant_count}/{k} = {precision_at_k:.2f} {contributes}\")\n",
    "    if is_relevant:\n",
    "        precisions_bad.append(precision_at_k)\n",
    "\n",
    "total_relevant = sum(bad_ranking)\n",
    "context_precision_bad = sum(precisions_bad) / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "print(f\"\\n   Sum of contributing precisions: {sum(precisions_bad):.2f}\")\n",
    "print(f\"   Total relevant items: {total_relevant}\")\n",
    "print(f\"\\n   ğŸ“Š Context Precision (Bad Ranking): {context_precision_bad:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "795ebbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“Š RANKING COMPARISON\n",
      "============================================================\n",
      "\n",
      "GOOD RANKING (Score: 1.00)          BAD RANKING (Score: 0.42)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ 1. âœ… Eiffel Tower Paris â”‚          â”‚ 1. âŒ Pizza Italy        â”‚\n",
      "â”‚ 2. âœ… Paris is capital   â”‚          â”‚ 2. âŒ Built in 1889      â”‚\n",
      "â”‚ 3. âŒ Built in 1889      â”‚          â”‚ 3. âœ… Paris is capital   â”‚\n",
      "â”‚ 4. âŒ Pizza Italy        â”‚          â”‚ 4. âœ… Eiffel Tower Paris â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "     Relevant at TOP! âœ“                  Relevant at BOTTOM! âœ—\n",
      "\n",
      "   Difference: 0.58\n",
      "   Same chunks, different ranking â†’ HUGE difference in score!\n"
     ]
    }
   ],
   "source": [
    "# Visual comparison\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š RANKING COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "GOOD RANKING (Score: {:.2f})          BAD RANKING (Score: {:.2f})\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 1. âœ… Eiffel Tower Paris â”‚          â”‚ 1. âŒ Pizza Italy        â”‚\n",
    "â”‚ 2. âœ… Paris is capital   â”‚          â”‚ 2. âŒ Built in 1889      â”‚\n",
    "â”‚ 3. âŒ Built in 1889      â”‚          â”‚ 3. âœ… Paris is capital   â”‚\n",
    "â”‚ 4. âŒ Pizza Italy        â”‚          â”‚ 4. âœ… Eiffel Tower Paris â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     Relevant at TOP! âœ“                  Relevant at BOTTOM! âœ—\n",
    "\"\"\".format(context_precision_good, context_precision_bad))\n",
    "\n",
    "print(f\"   Difference: {context_precision_good - context_precision_bad:.2f}\")\n",
    "print(\"   Same chunks, different ranking â†’ HUGE difference in score!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cad42236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ RAGAS Context Precision Results\n",
      "==================================================\n",
      "\n",
      "   Good Ranking (relevant at top): 1.00\n",
      "   Bad Ranking (relevant at bottom): 1.00\n",
      "\n",
      "   Difference: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Run actual RAGAS Context Precision\n",
    "\n",
    "# Good ranking sample\n",
    "good_sample = SingleTurnSample(\n",
    "    user_input=question,\n",
    "    reference=reference,\n",
    "    retrieved_contexts=[\n",
    "        \"The Eiffel Tower is located in Paris, France.\",\n",
    "        \"Paris is the capital of France.\",\n",
    "        \"The tower was built in 1889.\",\n",
    "        \"Pizza originated in Italy.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Bad ranking sample (same chunks, reversed order)\n",
    "bad_sample = SingleTurnSample(\n",
    "    user_input=question,\n",
    "    reference=reference,\n",
    "    retrieved_contexts=[\n",
    "        \"Pizza originated in Italy.\",\n",
    "        \"The tower was built in 1889.\",\n",
    "        \"Paris is the capital of France.\",\n",
    "        \"The Eiffel Tower is located in Paris, France.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "precision_metric = LLMContextPrecisionWithReference(llm=ragas_llm)\n",
    "\n",
    "good_score = run_async(precision_metric.single_turn_ascore(good_sample))\n",
    "bad_score = run_async(precision_metric.single_turn_ascore(bad_sample))\n",
    "\n",
    "print(\"ğŸ”¬ RAGAS Context Precision Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Good Ranking (relevant at top): {good_score:.2f}\")\n",
    "print(f\"   Bad Ranking (relevant at bottom): {bad_score:.2f}\")\n",
    "print(f\"\\n   Difference: {good_score - bad_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00ca8c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Reference Answer (Ground Truth):\n",
      "   'The Eiffel Tower is located in Paris. It was built in 1889. It is 330 meters tall.'\n",
      "\n",
      "ğŸ“š Retrieved Context:\n",
      "   1. 'The Eiffel Tower is a landmark located in Paris, France.'\n",
      "   2. 'The tower was completed in 1889 for the World's Fair.'\n"
     ]
    }
   ],
   "source": [
    "# Context Recall example setup\n",
    "\n",
    "recall_question = \"Tell me about the Eiffel Tower.\"\n",
    "recall_reference = \"The Eiffel Tower is located in Paris. It was built in 1889. It is 330 meters tall.\"\n",
    "\n",
    "# Retrieved context (missing the height information)\n",
    "recall_context = [\n",
    "    \"The Eiffel Tower is a landmark located in Paris, France.\",\n",
    "    \"The tower was completed in 1889 for the World's Fair.\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ Reference Answer (Ground Truth):\")\n",
    "print(f\"   '{recall_reference}'\")\n",
    "print(\"\\nğŸ“š Retrieved Context:\")\n",
    "for i, ctx in enumerate(recall_context, 1):\n",
    "    print(f\"   {i}. '{ctx}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69b164c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” STEP 1: Reference Claims\n",
      "==================================================\n",
      "   1. The Eiffel Tower is located in Paris\n",
      "   2. It was built in 1889\n",
      "   3. It is 330 meters tall\n"
     ]
    }
   ],
   "source": [
    "# Extract claims from reference\n",
    "\n",
    "reference_claims = [\n",
    "    \"The Eiffel Tower is located in Paris\",\n",
    "    \"It was built in 1889\",\n",
    "    \"It is 330 meters tall\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ” STEP 1: Reference Claims\")\n",
    "print(\"=\" * 50)\n",
    "for i, claim in enumerate(reference_claims, 1):\n",
    "    print(f\"   {i}. {claim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9f91d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” STEP 2: Claim Attribution Check\n",
      "============================================================\n",
      "   âœ… Found: 'The Eiffel Tower is located in Paris'\n",
      "   âœ… Found: 'It was built in 1889'\n",
      "   âœ… Found: 'It is 330 meters tall'\n"
     ]
    }
   ],
   "source": [
    "# Check attribution of each claim\n",
    "\n",
    "attribution_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Can the following claim be attributed to (found in) the given context?\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Claim: {claim}\n",
    "\n",
    "Answer \"YES\" if the claim is supported by the context, \"NO\" if it cannot be found.\n",
    "\"\"\")\n",
    "\n",
    "attribution_chain = attribution_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"ğŸ” STEP 2: Claim Attribution Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "combined_context = \"\\n\".join(recall_context)\n",
    "attribution_results = []\n",
    "\n",
    "for claim in reference_claims:\n",
    "    result = attribution_chain.invoke({\n",
    "        \"context\": combined_context,\n",
    "        \"claim\": claim\n",
    "    })\n",
    "    found = \"YES\" in result.upper()\n",
    "    attribution_results.append(found)\n",
    "    status = \"âœ… Found\" if found else \"âŒ MISSING\"\n",
    "    print(f\"   {status}: '{claim}'\")\n",
    "    if not found:\n",
    "        print(f\"      âš ï¸ This information was NOT retrieved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0e89346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ STEP 3: Calculate Context Recall\n",
      "==================================================\n",
      "\n",
      "   Claims found in context: 3\n",
      "   Total claims in reference: 3\n",
      "\n",
      "   Formula: 3 / 3 = 1.00\n",
      "\n",
      "   ğŸ“Š Context Recall: 1.00\n",
      "\n",
      "   âš ï¸ Interpretation: 0% of required info was NOT retrieved!\n"
     ]
    }
   ],
   "source": [
    "# Calculate Context Recall\n",
    "\n",
    "claims_found = sum(attribution_results)\n",
    "total_claims = len(reference_claims)\n",
    "manual_recall = claims_found / total_claims\n",
    "\n",
    "print(\"ğŸ”¢ STEP 3: Calculate Context Recall\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Claims found in context: {claims_found}\")\n",
    "print(f\"   Total claims in reference: {total_claims}\")\n",
    "print(f\"\\n   Formula: {claims_found} / {total_claims} = {manual_recall:.2f}\")\n",
    "print(f\"\\n   ğŸ“Š Context Recall: {manual_recall:.2f}\")\n",
    "print(f\"\\n   âš ï¸ Interpretation: {100 - manual_recall*100:.0f}% of required info was NOT retrieved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ab5b709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ RAGAS Context Recall Result\n",
      "==================================================\n",
      "\n",
      "   Manual calculation: 1.00\n",
      "   RAGAS metric score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Verify with RAGAS\n",
    "\n",
    "recall_sample = SingleTurnSample(\n",
    "    user_input=recall_question,\n",
    "    response=\"The Eiffel Tower is in Paris and was built in 1889.\",\n",
    "    reference=recall_reference,\n",
    "    retrieved_contexts=recall_context\n",
    ")\n",
    "\n",
    "recall_metric = LLMContextRecall(llm=ragas_llm)\n",
    "ragas_recall = run_async(recall_metric.single_turn_ascore(recall_sample))\n",
    "\n",
    "print(\"ğŸ”¬ RAGAS Context Recall Result\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Manual calculation: {manual_recall:.2f}\")\n",
    "print(f\"   RAGAS metric score: {ragas_recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50c99f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Reference Answer:\n",
      "   'Albert Einstein developed the theory of relativity at Princeton University in 1905.'\n",
      "\n",
      "ğŸ“š Retrieved Context:\n",
      "   'Albert Einstein was a famous physicist who worked at Princeton.'\n"
     ]
    }
   ],
   "source": [
    "# Entity Recall example setup\n",
    "\n",
    "entity_reference = \"Albert Einstein developed the theory of relativity at Princeton University in 1905.\"\n",
    "entity_context = [\n",
    "    \"Albert Einstein was a famous physicist who worked at Princeton.\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ Reference Answer:\")\n",
    "print(f\"   '{entity_reference}'\")\n",
    "print(\"\\nğŸ“š Retrieved Context:\")\n",
    "print(f\"   '{entity_context[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a910debb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Entity Extraction\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ Reference Entities:\n",
      "Hereâ€™s a breakdown of the named entities extracted from the text, with their types:\n",
      "\n",
      "*   Albert Einstein â€“ PERSON\n",
      "*   Princeton University â€“ ORGANIZATION\n",
      "*   1905 â€“ DATE\n",
      "\n",
      "\n",
      "ğŸ“‹ Context Entities:\n",
      "Hereâ€™s the extracted named entities from the text, with their types:\n",
      "\n",
      "*   Albert Einstein â€“ PERSON\n",
      "*   physicist â€“ ORGANIZATION\n",
      "*   Princeton â€“ LOCATION\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manual entity extraction\n",
    "\n",
    "entity_extraction_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Extract all named entities from the following text. \n",
    "Include: PERSON, ORGANIZATION, LOCATION, DATE, and other proper nouns.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "List each entity on a new line with its type:\n",
    "\"\"\")\n",
    "\n",
    "entity_chain = entity_extraction_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"ğŸ” Entity Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ“‹ Reference Entities:\")\n",
    "ref_entities = entity_chain.invoke({\"text\": entity_reference})\n",
    "print(ref_entities)\n",
    "\n",
    "print(\"\\nğŸ“‹ Context Entities:\")\n",
    "ctx_entities = entity_chain.invoke({\"text\": entity_context[0]})\n",
    "print(ctx_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a159542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Entity Comparison\n",
      "============================================================\n",
      "\n",
      "| Entity in Reference | Type | Found in Context? |\n",
      "|--------------------|--------------|------------------|\n",
      "| Albert Einstein    | PERSON       | âœ… Yes            |\n",
      "| Princeton University | ORGANIZATION | âœ… Yes            |\n",
      "| 1905               | DATE         | âŒ MISSING        |\n",
      "\n",
      "ğŸ“Š Entity Recall: 2/3 = 0.67\n",
      "âš ï¸ Missing: '1905' - Critical date not retrieved!\n"
     ]
    }
   ],
   "source": [
    "# Manual entity analysis\n",
    "\n",
    "# Define entities for analysis\n",
    "reference_entities = {\n",
    "    \"Albert Einstein\": \"PERSON\",\n",
    "    \"Princeton University\": \"ORGANIZATION\",\n",
    "    \"1905\": \"DATE\"\n",
    "}\n",
    "\n",
    "context_entities = {\n",
    "    \"Albert Einstein\": \"PERSON\",\n",
    "    \"Princeton\": \"ORGANIZATION\"  # Partial match\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š Entity Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n| Entity in Reference | Type | Found in Context? |\")\n",
    "print(\"|\" + \"-\" * 20 + \"|\" + \"-\" * 14 + \"|\" + \"-\" * 18 + \"|\")\n",
    "\n",
    "found_count = 0\n",
    "for entity, entity_type in reference_entities.items():\n",
    "    # Check if entity (or partial) exists in context\n",
    "    found = any(entity.lower() in ctx.lower() or ctx.lower() in entity.lower() \n",
    "                for ctx in context_entities.keys())\n",
    "    if found:\n",
    "        found_count += 1\n",
    "    status = \"âœ… Yes\" if found else \"âŒ MISSING\"\n",
    "    print(f\"| {entity:18} | {entity_type:12} | {status:16} |\")\n",
    "\n",
    "entity_recall = found_count / len(reference_entities)\n",
    "print(f\"\\nğŸ“Š Entity Recall: {found_count}/{len(reference_entities)} = {entity_recall:.2f}\")\n",
    "print(f\"âš ï¸ Missing: '1905' - Critical date not retrieved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17baabf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ RAGAS Context Entity Recall Result\n",
      "==================================================\n",
      "\n",
      "   Manual estimate: 0.67\n",
      "   RAGAS metric score: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Verify with RAGAS\n",
    "\n",
    "entity_sample = SingleTurnSample(\n",
    "    reference=entity_reference,\n",
    "    retrieved_contexts=entity_context\n",
    ")\n",
    "\n",
    "entity_metric = ContextEntityRecall(llm=ragas_llm)\n",
    "ragas_entity_recall = run_async(entity_metric.single_turn_ascore(entity_sample))\n",
    "\n",
    "print(\"ğŸ”¬ RAGAS Context Entity Recall Result\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Manual estimate: {entity_recall:.2f}\")\n",
    "print(f\"   RAGAS metric score: {ragas_entity_recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce0dd79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Question: 'What is LIC known for?'\n",
      "\n",
      "ğŸ“ Response to evaluate:\n",
      "   'LIC is the largest insurance company in India, known for its vast portfolio. LIC contributes to financial stability.'\n",
      "\n",
      "ğŸ“ Reference (Ground Truth):\n",
      "   'LIC is the largest insurance company in India, established in 1956. It is known for managing a large portfolio of investments.'\n",
      "\n",
      "ğŸ“š Retrieved Contexts:\n",
      "   1. 'LIC was established in 1956 following nationalization....' âœ…\n",
      "   2. 'LIC is the largest insurance company with huge investments....' âœ…\n",
      "   3. 'LIC manages substantial funds for financial stability....' âœ…\n",
      "   4. 'The Indian economy is one of the fastest-growing economies.....' â† NOISE!\n"
     ]
    }
   ],
   "source": [
    "# Noise Sensitivity example setup\n",
    "\n",
    "noise_question = \"What is LIC known for?\"\n",
    "noise_response = \"LIC is the largest insurance company in India, known for its vast portfolio. LIC contributes to financial stability.\"\n",
    "noise_reference = \"LIC is the largest insurance company in India, established in 1956. It is known for managing a large portfolio of investments.\"\n",
    "\n",
    "noise_contexts = [\n",
    "    \"LIC was established in 1956 following nationalization.\",           # âœ… Relevant\n",
    "    \"LIC is the largest insurance company with huge investments.\",      # âœ… Relevant\n",
    "    \"LIC manages substantial funds for financial stability.\",           # âœ… Relevant\n",
    "    \"The Indian economy is one of the fastest-growing economies...\"     # âŒ NOISE!\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ Question: '{}'\\n\".format(noise_question))\n",
    "print(\"ğŸ“ Response to evaluate:\")\n",
    "print(f\"   '{noise_response}'\")\n",
    "print(\"\\nğŸ“ Reference (Ground Truth):\")\n",
    "print(f\"   '{noise_reference}'\")\n",
    "print(\"\\nğŸ“š Retrieved Contexts:\")\n",
    "for i, ctx in enumerate(noise_contexts, 1):\n",
    "    noise_tag = \" â† NOISE!\" if i == 4 else \" âœ…\"\n",
    "    print(f\"   {i}. '{ctx[:60]}...'{noise_tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9b27c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Claim Analysis\n",
      "======================================================================\n",
      "\n",
      "| Claim | Correct? | Reason |\n",
      "|---------------------------------------------|----------|----------------------------------------|\n",
      "| LIC is the largest insurance company in Ind | âœ… Yes    | Matches reference                      |\n",
      "| LIC is known for its vast portfolio         | âœ… Yes    | Matches reference (portfolio)          |\n",
      "| LIC contributes to financial stability      | âŒ No     | NOT in reference - possible hallucinat |\n"
     ]
    }
   ],
   "source": [
    "# Analyze claims in response\n",
    "\n",
    "response_claims = [\n",
    "    (\"LIC is the largest insurance company in India\", True, \"Matches reference\"),\n",
    "    (\"LIC is known for its vast portfolio\", True, \"Matches reference (portfolio)\"),\n",
    "    (\"LIC contributes to financial stability\", False, \"NOT in reference - possible hallucination from noise!\")\n",
    "]\n",
    "\n",
    "print(\"ğŸ” Claim Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n| Claim | Correct? | Reason |\")\n",
    "print(\"|\" + \"-\" * 45 + \"|\" + \"-\" * 10 + \"|\" + \"-\" * 40 + \"|\")\n",
    "\n",
    "incorrect_count = 0\n",
    "for claim, is_correct, reason in response_claims:\n",
    "    status = \"âœ… Yes\" if is_correct else \"âŒ No\"\n",
    "    if not is_correct:\n",
    "        incorrect_count += 1\n",
    "    print(f\"| {claim[:43]:43} | {status:8} | {reason[:38]:38} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2c1c59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ Noise Sensitivity Calculation\n",
      "==================================================\n",
      "\n",
      "   Incorrect claims: 1\n",
      "   Total claims: 3\n",
      "\n",
      "   Formula: 1 / 3 = 0.33\n",
      "\n",
      "   ğŸ“Š Noise Sensitivity: 0.33\n",
      "   âš ï¸ Warning! Model is sometimes confused by noise.\n"
     ]
    }
   ],
   "source": [
    "# Calculate Noise Sensitivity\n",
    "\n",
    "total_claims = len(response_claims)\n",
    "noise_sensitivity = incorrect_count / total_claims\n",
    "\n",
    "print(\"ğŸ”¢ Noise Sensitivity Calculation\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Incorrect claims: {incorrect_count}\")\n",
    "print(f\"   Total claims: {total_claims}\")\n",
    "print(f\"\\n   Formula: {incorrect_count} / {total_claims} = {noise_sensitivity:.2f}\")\n",
    "print(f\"\\n   ğŸ“Š Noise Sensitivity: {noise_sensitivity:.2f}\")\n",
    "\n",
    "if noise_sensitivity < 0.3:\n",
    "    print(\"   âœ… Good! Model is mostly resistant to noise.\")\n",
    "elif noise_sensitivity < 0.6:\n",
    "    print(\"   âš ï¸ Warning! Model is sometimes confused by noise.\")\n",
    "else:\n",
    "    print(\"   ğŸš¨ Bad! Model is highly susceptible to noise.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "043bface",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ RAGAS Noise Sensitivity Result\n",
      "==================================================\n",
      "\n",
      "   Mode: relevant\n",
      "   Score: 0.00\n",
      "\n",
      "   Remember: Lower is better for this metric!\n"
     ]
    }
   ],
   "source": [
    "# Verify with RAGAS (both modes)\n",
    "\n",
    "noise_sample = SingleTurnSample(\n",
    "    user_input=noise_question,\n",
    "    response=noise_response,\n",
    "    reference=noise_reference,\n",
    "    retrieved_contexts=noise_contexts\n",
    ")\n",
    "\n",
    "# Relevant mode: errors from relevant contexts\n",
    "noise_metric_relevant = NoiseSensitivity(llm=ragas_llm, mode=\"relevant\")\n",
    "\n",
    "ragas_noise = run_async(noise_metric_relevant.single_turn_ascore(noise_sample))\n",
    "\n",
    "print(\"ğŸ”¬ RAGAS Noise Sensitivity Result\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Mode: relevant\")\n",
    "print(f\"   Score: {ragas_noise:.2f}\")\n",
    "print(f\"\\n   Remember: Lower is better for this metric!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86952174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
