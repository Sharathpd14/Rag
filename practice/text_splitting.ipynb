{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b49c3e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment ready\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Environment ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d603f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Original document: 8639 characters\n",
      "\n",
      "âœ‚ï¸ Split into 12 chunks\n",
      "\n",
      "======================================================================\n",
      "Chunk 1 (991 chars):\n",
      "======================================================================\n",
      "LANGCHAIN STUDY NOTES - RAG IMPLEMENTATION\n",
      "==========================================\n",
      "\n",
      "Date: January 15, 2025\n",
      "Topic: Retrieval-Augmented Generation with LangChain 1.0+\n",
      "\n",
      "\n",
      "CORE CONCEPTS\n",
      "-------------\n",
      "\n",
      "1. Document Object Structure\n",
      "   - page_content: The actual text content\n",
      "   - metadata: Dictionary wit...\n",
      "\n",
      "======================================================================\n",
      "Chunk 2 (809 chars):\n",
      "======================================================================\n",
      "TEXT SPLITTING STRATEGIES\n",
      "--------------------------\n",
      "\n",
      "RecursiveCharacterTextSplitter (RECOMMENDED)\n",
      "- Tries to split on semantic boundaries\n",
      "- Order: double newline Ã¢â€ â€™ newline Ã¢â€ â€™ period Ã¢â€ â€™ space Ã¢â€ â€™ character\n",
      "- Best for general text and documentation\n",
      "- Configuration: chunk_size=1000, chunk_overlap=...\n",
      "\n",
      "======================================================================\n",
      "Chunk 3 (864 chars):\n",
      "======================================================================\n",
      "TokenTextSplitter\n",
      "- Splits based on token count, not characters\n",
      "- More accurate for LLM context window limits\n",
      "- Uses tiktoken for OpenAI models\n",
      "\n",
      "\n",
      "CHUNK SIZE GUIDELINES\n",
      "----------------------\n",
      "\n",
      "Content Type          | Chunk Size | Overlap | Notes\n",
      "----------------------|------------|---------|---------...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load our sample text\n",
    "txt_path = \"../simple-rag-langchain/sample_data/notes.txt\"\n",
    "\n",
    "if Path(txt_path).exists():\n",
    "    # Load the document\n",
    "    loader = TextLoader(txt_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"ðŸ“„ Original document: {len(documents[0].page_content)} characters\\n\")\n",
    "    \n",
    "    # Create splitter\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,        # Maximum chunk size in characters\n",
    "        chunk_overlap=200,      # Overlap between chunks\n",
    "        length_function=len,    # How to measure length\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Try these in order\n",
    "    )\n",
    "    \n",
    "    # Split the document\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"âœ‚ï¸ Split into {len(chunks)} chunks\\n\")\n",
    "    \n",
    "    # Examine first 3 chunks\n",
    "    for i, chunk in enumerate(chunks[:3], 1):\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Chunk {i} ({len(chunk.page_content)} chars):\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(chunk.page_content[:300] + \"...\" if len(chunk.page_content) > 300 else chunk.page_content)\n",
    "        print()\n",
    "else:\n",
    "    print(f\"âŒ File not found: {txt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5856b22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Examining overlap between chunks:\n",
      "\n",
      "Chunk 1 ending:\n",
      "  ...ontent: The actual text content\n",
      "   - metadata: Dictionary with additional information (source, page, date, etc.)\n",
      "   - id: Unique identifier (optional)\n",
      "\n",
      "Chunk 2 beginning:\n",
      "  2. LCEL (LangChain Expression Language)\n",
      "   - Uses pipe operator | to chain components\n",
      "   - More readable than nested function calls\n",
      "   - Better error ...\n",
      "\n",
      "ðŸ’¡ Notice the overlap? This preserves context!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate overlap\n",
    "if Path(txt_path).exists():\n",
    "    docs = TextLoader(txt_path).load()\n",
    "    \n",
    "    # Splitter with overlap\n",
    "    splitter_with_overlap = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100  # 100 chars overlap\n",
    "    )\n",
    "    \n",
    "    chunks = splitter_with_overlap.split_documents(docs)\n",
    "    \n",
    "    print(\"ðŸ” Examining overlap between chunks:\\n\")\n",
    "    \n",
    "    # Show overlap between chunk 1 and 2\n",
    "    if len(chunks) >= 2:\n",
    "        chunk1_end = chunks[0].page_content[-150:]\n",
    "        chunk2_start = chunks[1].page_content[:150]\n",
    "        \n",
    "        print(\"Chunk 1 ending:\")\n",
    "        print(f\"  ...{chunk1_end}\")\n",
    "        print(\"\\nChunk 2 beginning:\")\n",
    "        print(f\"  {chunk2_start}...\")\n",
    "        print(\"\\nðŸ’¡ Notice the overlap? This preserves context!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c2133f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸ Split code into 3 chunks:\n",
      "\n",
      "Chunk 1:\n",
      "def calculate_total(items):\n",
      "    \"\"\"Calculate total price of items.\"\"\"\n",
      "    total = 0\n",
      "    for item in items:\n",
      "        total += item['price']\n",
      "    return total\n",
      "--------------------------------------------------\n",
      "Chunk 2:\n",
      "def apply_discount(total, discount_percent):\n",
      "    \"\"\"Apply discount to total.\"\"\"\n",
      "    discount = total * (discount_percent / 100)\n",
      "    return total - discount\n",
      "--------------------------------------------------\n",
      "Chunk 3:\n",
      "class ShoppingCart:\n",
      "    def __init__(self):\n",
      "        self.items = []\n",
      "\n",
      "    def add_item(self, item):\n",
      "        self.items.append(item)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example: Splitting Python code\n",
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Python code example\n",
    "python_code = '''\n",
    "def calculate_total(items):\n",
    "    \"\"\"Calculate total price of items.\"\"\"\n",
    "    total = 0\n",
    "    for item in items:\n",
    "        total += item['price']\n",
    "    return total\n",
    "\n",
    "def apply_discount(total, discount_percent):\n",
    "    \"\"\"Apply discount to total.\"\"\"\n",
    "    discount = total * (discount_percent / 100)\n",
    "    return total - discount\n",
    "\n",
    "class ShoppingCart:\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "    \n",
    "    def add_item(self, item):\n",
    "        self.items.append(item)\n",
    "'''\n",
    "\n",
    "# Python-aware splitter\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "code_chunks = python_splitter.split_text(python_code)\n",
    "\n",
    "print(f\"âœ‚ï¸ Split code into {len(code_chunks)} chunks:\\n\")\n",
    "for i, chunk in enumerate(code_chunks, 1):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d9c6c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 3 chunks:\n",
      "\n",
      "Chunk 1: First paragraph about machine learning.\n",
      "It has multiple sentences. This is important context.\n",
      "\n",
      "Chunk 2: Second paragraph about deep learning.\n",
      "Neural networks are powerful. They learn from data.\n",
      "\n",
      "Chunk 3: Third paragraph about transformers.\n",
      "Attention mechanisms are key. They revolutionized NLP.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Sample text with clear paragraph breaks\n",
    "sample_text = \"\"\"First paragraph about machine learning.\n",
    "It has multiple sentences. This is important context.\n",
    "\n",
    "Second paragraph about deep learning.\n",
    "Neural networks are powerful. They learn from data.\n",
    "\n",
    "Third paragraph about transformers.\n",
    "Attention mechanisms are key. They revolutionized NLP.\n",
    "\"\"\"\n",
    "\n",
    "# Split on paragraph breaks\n",
    "simple_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",  # Split on double newline (paragraphs)\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = simple_splitter.split_text(sample_text)\n",
    "\n",
    "print(f\"Split into {len(chunks)} chunks:\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}: {chunk.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3821d8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸ Split HTML into 48 sections\n",
      "\n",
      "======================================================================\n",
      "Section 1:\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG'}\n",
      "Content (first 200 chars): Building Intelligent Applications with RAG...\n",
      "\n",
      "======================================================================\n",
      "Section 2:\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG'}\n",
      "Content (first 200 chars): | |  \n",
      "By Dr. Amanda Foster  \n",
      "January 15, 2025  \n",
      "12 min read...\n",
      "\n",
      "======================================================================\n",
      "Section 3:\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Introduction'}\n",
      "Content (first 200 chars): Introduction...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "# Load the HTML blog post\n",
    "html_path = \"../simple-rag-langchain/sample_data/blog_post.html\"\n",
    "\n",
    "if Path(html_path).exists():\n",
    "    # Read HTML content\n",
    "    with open(html_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "    \n",
    "    # Define headers to split on\n",
    "    headers_to_split_on = [\n",
    "        (\"h1\", \"Title\"),\n",
    "        (\"h2\", \"Section\"),\n",
    "        (\"h3\", \"Subsection\"),\n",
    "    ]\n",
    "    \n",
    "    # Create splitter\n",
    "    html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    \n",
    "    # Split the HTML\n",
    "    html_chunks = html_splitter.split_text(html_content)\n",
    "    \n",
    "    print(f\"âœ‚ï¸ Split HTML into {len(html_chunks)} sections\\n\")\n",
    "    \n",
    "    # Show first 3 sections with metadata\n",
    "    for i, chunk in enumerate(html_chunks[:3], 1):\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Section {i}:\")\n",
    "        print(f\"Metadata: {chunk.metadata}\")\n",
    "        print(f\"Content (first 200 chars): {chunk.page_content[:200]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(f\"âŒ HTML file not found: {html_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c47e4497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸ Split JSON into 7 chunks\n",
      "\n",
      "First chunk:\n",
      "\"{\\\"api_version\\\": \\\"v2.0\\\", \\\"timestamp\\\": \\\"2025-01-15T10:30:00Z\\\", \\\"total_results\\\": 5, \\\"articles\\\": {\\\"0\\\": {\\\"id\\\": \\\"article_001\\\", \\\"title\\\": \\\"Introduction to Retrieval-Augmented Generation (RAG)\\\", \\\"author\\\": \\\"Dr. Sarah Chen\\\", \\\"published_date\\\": \\\"2025-01-10\\\", \\\"category\\\": \\\"Machine Learning\\\", \\\"tags\\\": {\\\"0\\\": \\\"RAG\\\", \\\"1\\\": \\\"LLM\\\", \\\"2\\\": \\\"NLP\\\", \\\"3\\\": \\\"AI\\\"}, \\\"summary\\\": \\\"Retrieval-Augmented Generation (RAG) is a powerful technique that combines information retrieval ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "import json\n",
    "\n",
    "# Load JSON data\n",
    "json_path = \"../simple-rag-langchain/sample_data/api_response.json\"\n",
    "\n",
    "if Path(json_path).exists():\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    # Create splitter\n",
    "    json_splitter = RecursiveJsonSplitter(\n",
    "        max_chunk_size=1000,\n",
    "        min_chunk_size=100\n",
    "    )\n",
    "    \n",
    "    # Split\n",
    "    json_chunks = json_splitter.split_text(\n",
    "        json_data=json_data,\n",
    "        convert_lists=True\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ‚ï¸ Split JSON into {len(json_chunks)} chunks\\n\")\n",
    "    \n",
    "    # Show first chunk\n",
    "    print(\"First chunk:\")\n",
    "    print(json.dumps(json_chunks[0], indent=2)[:500] + \"...\")\n",
    "else:\n",
    "    print(f\"âŒ JSON file not found: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e15c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 1 token-based chunks:\n",
      "\n",
      "Chunk 1: The transformer architecture, introduced in the paper 'Attention Is All You Need', \n",
      "revolutionized natural language processing. It uses self-attention mechanisms to process \n",
      "sequences in parallel, making it much faster than recurrent neural networks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "# Sample text\n",
    "\n",
    "## \"\"pypi tiktoken\"\" -> search this.\n",
    "\n",
    "\n",
    "text = \"\"\"The transformer architecture, introduced in the paper 'Attention Is All You Need', \n",
    "revolutionized natural language processing. It uses self-attention mechanisms to process \n",
    "sequences in parallel, making it much faster than recurrent neural networks.\"\"\"\n",
    "\n",
    "# Token-based splitter\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=50,  # 50 tokens (not characters!)\n",
    "    chunk_overlap=10,\n",
    "    encoding_name=\"cl100k_base\"  # GPT-3.5/GPT-4 tokenizer\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(text)\n",
    "\n",
    "print(f\"Split into {len(token_chunks)} token-based chunks:\\n\")\n",
    "for i, chunk in enumerate(token_chunks, 1):\n",
    "    print(f\"Chunk {i}: {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d681220e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Chunk Size Comparison:\n",
      "\n",
      "Size     Chunks     Avg Length   Overlap %\n",
      "--------------------------------------------------\n",
      "500      26         347          20%\n",
      "1000     12         825          20%\n",
      "1500     8          1238         20%\n",
      "2000     6          1642         20%\n"
     ]
    }
   ],
   "source": [
    "# Compare different chunk sizes\n",
    "if Path(txt_path).exists():\n",
    "    docs = TextLoader(txt_path).load()\n",
    "    \n",
    "    chunk_sizes = [500, 1000, 1500, 2000]\n",
    "    \n",
    "    print(\"ðŸ“Š Chunk Size Comparison:\\n\")\n",
    "    print(f\"{'Size':<8} {'Chunks':<10} {'Avg Length':<12} {'Overlap %'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for size in chunk_sizes:\n",
    "        overlap = int(size * 0.2)  # 20% overlap\n",
    "        \n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=size,\n",
    "            chunk_overlap=overlap\n",
    "        )\n",
    "        \n",
    "        chunks = splitter.split_documents(docs)\n",
    "        avg_length = sum(len(c.page_content) for c in chunks) / len(chunks)\n",
    "        overlap_pct = (overlap / size) * 100\n",
    "        \n",
    "        print(f\"{size:<8} {len(chunks):<10} {avg_length:<12.0f} {overlap_pct:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dba47e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
