{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5986895c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports successful!\n",
      "‚úì Ready for local offline RAG!\n",
      "\n",
      "Python version: 3.12.10 (main, Apr  9 2025, 04:06:22) [MSC v.1943 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# LangChain Document Loaders\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# LangChain Text Splitters\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Ollama Integration\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "\n",
    "# ChromaDB Vector Store\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# LangChain Core Components\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\"‚úì All imports successful!\")\n",
    "print(\"‚úì Ready for local offline RAG!\")\n",
    "print(f\"\\nPython version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "214bb12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                     ID              SIZE      MODIFIED    \n",
      "gemma3:1b                8648f39daa8f    815 MB    3 weeks ago    \n",
      "nomic-embed-text:v1.5    0a109f422b47    274 MB    3 weeks ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecfa84f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Ollama connection...\n",
      "\n",
      "‚úì Ollama is working!\n",
      "Response: Hello! I am running locally on your machine! üòä\n",
      "\n",
      "Ollama connection is successfull\n"
     ]
    }
   ],
   "source": [
    "# Test Ollama connection with a simple query\n",
    "print(\"Testing Ollama connection...\\n\")\n",
    "\n",
    "try:\n",
    "    test_llm = ChatOllama(model=\"gemma3:1b\", temperature=0)\n",
    "    response = test_llm.invoke(\"Say 'Hello! I am running locally on your machine!'\")\n",
    "    \n",
    "    print(\"‚úì Ollama is working!\")\n",
    "    print(f\"Response: {response.content}\")\n",
    "    \n",
    "    print(\"Ollama connection is successfull\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error connecting to Ollama: {e}\")\n",
    "    print(\"\\nMake sure Ollama is running. Try: ollama serve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ac397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 15 pages from 'attention.pdf'\n",
      "\n",
      "--- First Page Preview ---\n",
      "Content (first 300 chars): Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Par...\n",
      "\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Total characters: 39,587\n"
     ]
    }
   ],
   "source": [
    "# ===== CONFIGURATION: Update this path to your PDF file =====\n",
    "pdf_path = \"attention.pdf\"  # Change this to your PDF file path\n",
    "# =============================================================\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"‚ö†Ô∏è  ERROR: File '{pdf_path}' not found!\")\n",
    "    print(\"Please update the pdf_path variable with your PDF file location.\")\n",
    "else:\n",
    "    # Load the PDF\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Display information\n",
    "    print(f\"‚úì Loaded {len(documents)} pages from '{pdf_path}'\")\n",
    "    print(f\"\\n--- First Page Preview ---\")\n",
    "    print(f\"Content (first 300 chars): {documents[0].page_content[:300]}...\")\n",
    "    print(f\"\\nMetadata: {documents[0].metadata}\")\n",
    "    print(f\"\\nTotal characters: {sum(len(doc.page_content) for doc in documents):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd27dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Split 15 documents into 49 chunks\n",
      "\n",
      "Average chunk size: 873 characters\n",
      "\n",
      "--- Chunk Examples ---\n",
      "\n",
      "Chunk 1 (length: 986 chars):\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n",
      "\n",
      "Chunk 2 (length: 944 chars):\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more pa...\n",
      "\n",
      "Chunk 3 (length: 986 chars):\n",
      "‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Tra...\n"
     ]
    }
   ],
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,        # Characters per chunk\n",
    "    chunk_overlap=128,      # Overlap to maintain context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Split on paragraphs, then lines, etc.\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Display results\n",
    "avg_chunk_size = sum(len(chunk.page_content) for chunk in chunks) / len(chunks)\n",
    "\n",
    "print(f\"‚úì Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "print(f\"\\nAverage chunk size: {avg_chunk_size:.0f} characters\")\n",
    "\n",
    "# Preview chunks\n",
    "print(f\"\\n--- Chunk Examples ---\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} (length: {len(chunk.page_content)} chars):\")\n",
    "    print(f\"{chunk.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74ba0be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing nomic-embed-text embeddings...\n",
      "\n",
      "‚úì Embeddings model: nomic-embed-text\n",
      "‚úì Embedding dimension: 768\n",
      "‚úì Sample embedding (first 10 values): [0.03250689, 0.06084158, -0.16616665, -0.08210022, 0.043314006, -0.025992092, 0.051577497, -0.015190071, -0.0082475105, -0.028356079]\n",
      "\n",
      "‚ÑπÔ∏è  Each chunk will be converted to a 768-dimensional vector\n",
      "‚ÑπÔ∏è  All processing happens locally on your machine!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ollama Embeddings with nomic-embed-text\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text:v1.5\",\n",
    "    # base_url=\"http://localhost:11434\"  # Default Ollama URL\n",
    ")\n",
    "\n",
    "# Test embeddings\n",
    "print(\"Testing nomic-embed-text embeddings...\\n\")\n",
    "sample_text = \"This is a test sentence for embeddings.\"\n",
    "sample_embedding = embeddings.embed_query(sample_text)\n",
    "\n",
    "print(f\"‚úì Embeddings model: nomic-embed-text\")\n",
    "print(f\"‚úì Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"‚úì Sample embedding (first 10 values): {sample_embedding[:10]}\")\n",
    "print(f\"\\n‚ÑπÔ∏è  Each chunk will be converted to a {len(sample_embedding)}-dimensional vector\")\n",
    "print(f\"‚ÑπÔ∏è  All processing happens locally on your machine!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c89f9c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ChromaDB vector store from 49 chunks...\n",
      "This may take a minute...\n",
      "\n",
      "‚úì ChromaDB vector store created successfully!\n",
      "‚úì Indexed 49 document chunks\n",
      "‚úì Stored at: ./chroma_db\n",
      "\n",
      "‚ÑπÔ∏è  Vector store persisted to disk - you can reload it later!\n"
     ]
    }
   ],
   "source": [
    "# Create ChromaDB vector store\n",
    "print(f\"Creating ChromaDB vector store from {len(chunks)} chunks...\")\n",
    "print(\"This may take a minute...\\n\")\n",
    "\n",
    "# Set persistent directory\n",
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=\"local_rag_collection\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì ChromaDB vector store created successfully!\")\n",
    "print(f\"‚úì Indexed {len(chunks)} document chunks\")\n",
    "print(f\"‚úì Stored at: {persist_directory}\")\n",
    "print(f\"\\n‚ÑπÔ∏è  Vector store persisted to disk - you can reload it later!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f73e04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Retriever configured successfully\n",
      "  - Search type: similarity\n",
      "  - Number of documents to retrieve (k): 4\n",
      "\n",
      "--- Retriever Test ---\n",
      "Query: 'What is the main topic of this document?'\n",
      "\n",
      "Retrieved 4 documents:\n",
      "\n",
      "Document 1:\n",
      "  Content preview: (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "remained unchanged from the English-to-German base...\n",
      "  Source: Page 8\n",
      "\n",
      "Document 2:\n",
      "  Content preview: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "...\n",
      "  Source: Page 13\n",
      "\n",
      "Document 3:\n",
      "  Content preview: Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for different layer types. n is the sequence length, d...\n",
      "  Source: Page 5\n",
      "\n",
      "Document 4:\n",
      "  Content preview: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "re...\n",
      "  Source: Page 12\n"
     ]
    }
   ],
   "source": [
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",    # Use cosine similarity\n",
    "    search_kwargs={\"k\": 4}        # Retrieve top 4 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"‚úì Retriever configured successfully\")\n",
    "print(f\"  - Search type: similarity\")\n",
    "print(f\"  - Number of documents to retrieve (k): 4\")\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"What is the main topic of this document?\"\n",
    "print(f\"\\n--- Retriever Test ---\")\n",
    "print(f\"Query: '{test_query}'\")\n",
    "\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "\n",
    "print(f\"\\nRetrieved {len(retrieved_docs)} documents:\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Content preview: {doc.page_content[:150]}...\")\n",
    "    print(f\"  Source: Page {doc.metadata.get('page', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20ca2f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LLM configured successfully\n",
      "  - Model: gemma3:1b (local)\n",
      "  - Temperature: 0 (deterministic)\n",
      "\n",
      "LLM Test Response: Hello! I am Gemma running locally! üòä\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ollama LLM\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:1b\",\n",
    "    temperature=0,          # Deterministic responses (0 = focused, 1 = creative)\n",
    "    # num_predict=2000,     # Max tokens to generate\n",
    "    # top_k=40,             # Top-k sampling\n",
    "    # top_p=0.9,            # Top-p (nucleus) sampling\n",
    ")\n",
    "\n",
    "print(\"‚úì LLM configured successfully\")\n",
    "print(f\"  - Model: gemma3:1b (local)\")\n",
    "print(f\"  - Temperature: 0 (deterministic)\")\n",
    "\n",
    "# Test LLM\n",
    "test_response = llm.invoke(\"Say 'Hello! I am Gemma running locally!'\")\n",
    "print(f\"\\nLLM Test Response: {test_response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e40cf153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì RAG chain created successfully using LCEL!\n",
      "\n",
      "RAG Pipeline Flow:\n",
      "  1. User provides a query\n",
      "  2. Retriever finds top 4 relevant chunks (local ChromaDB)\n",
      "  3. Chunks are formatted as context\n",
      "  4. Context + question formatted with prompt template\n",
      "  5. Local LLM (gemma3:1b) generates answer\n",
      "  6. Answer parsed and returned\n",
      "\n",
      "üîí Everything runs locally on your machine!\n"
     ]
    }
   ],
   "source": [
    "# Define prompt template\n",
    "system_prompt = (\n",
    "    \"You are a helpful assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer based on the context, say that you don't know. \"\n",
    "    \"Keep the answer concise and accurate.\\n\\n\"\n",
    "    \"Context: {context}\\n\\n\"\n",
    "    \"Question: {question}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format retrieved documents into a single string.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build RAG chain using LCEL\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,  # Retrieve and format docs\n",
    "        \"question\": RunnablePassthrough()      # Pass through the question\n",
    "    }\n",
    "    | prompt           # Format with prompt template\n",
    "    | llm              # Generate answer with local LLM\n",
    "    | StrOutputParser() # Parse output to string\n",
    ")\n",
    "\n",
    "print(\"‚úì RAG chain created successfully using LCEL!\")\n",
    "print(\"\\nRAG Pipeline Flow:\")\n",
    "print(\"  1. User provides a query\")\n",
    "print(\"  2. Retriever finds top 4 relevant chunks (local ChromaDB)\")\n",
    "print(\"  3. Chunks are formatted as context\")\n",
    "print(\"  4. Context + question formatted with prompt template\")\n",
    "print(\"  5. Local LLM (gemma3:1b) generates answer\")\n",
    "print(\"  6. Answer parsed and returned\")\n",
    "print(\"\\nüîí Everything runs locally on your machine!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41ae6ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the main topic or contribution of this document?\n",
      "\n",
      "Processing locally...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The document discusses the parameters used during the development of the Section 22 development set, specifically focusing on learning rates and beam size.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "SOURCE DOCUMENTS USED:\n",
      "================================================================================\n",
      "\n",
      "Document 1:\n",
      "  Page: 8\n",
      "  Content: (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "remained unchanged from the English-to-German base translation model. During inference, we\n",
      "9...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "  Page: 12\n",
      "  Content: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 3:\n",
      "  Page: 13\n",
      "  Content: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "sh...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 4:\n",
      "  Page: 2\n",
      "  Content: Decoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "att...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example Query 1: General question\n",
    "query1 = \"What is the main topic or contribution of this document?\"\n",
    "\n",
    "print(f\"Query: {query1}\")\n",
    "print(\"\\nProcessing locally...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(query1)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Show source documents\n",
    "print(\"\\nSOURCE DOCUMENTS USED:\")\n",
    "print(\"=\" * 80)\n",
    "retrieved_docs = retriever.invoke(query1)\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Page: {doc.metadata.get('page', 'N/A')}\")\n",
    "    print(f\"  Content: {doc.page_content[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98d528ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Can you summarize the key technical contributions or innovations mentioned?\n",
      "\n",
      "Processing locally...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "Here‚Äôs a summary of the key technical contributions:\n",
      "\n",
      "*   **Transformer Model Development:** Ashish, with Illia, designed and implemented the first Transformer models.\n",
      "*   **Attention Mechanism:** Noam proposed scaled dot-product attention and parameter-free position representation.\n",
      "*   **Tensor2Tensor:** Niki designed, implemented, tuned, and evaluated numerous model variants.\n",
      "*   **Position Representation:** Llion experimented with novel model variants.\n",
      "*   **Training Techniques:** The team used techniques like beam search and parameter averaging.\n",
      "*   **Evaluation:** The team estimated the number of floating-point operations used for training models.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 2: Specific information extraction\n",
    "query2 = \"Can you summarize the key technical contributions or innovations mentioned?\"\n",
    "\n",
    "print(f\"Query: {query2}\")\n",
    "print(\"\\nProcessing locally...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(query2)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0709b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to compare embedding models\n",
    "\n",
    "# print(\"Comparing embedding models...\\n\")\n",
    "\n",
    "# test_query = \"What is attention mechanism?\"\n",
    "\n",
    "# # Test with nomic-embed-text\n",
    "# print(\"=\" * 80)\n",
    "# print(\"Using nomic-embed-text:\")\n",
    "# print(\"=\" * 80)\n",
    "# embeddings_nomic = OllamaEmbeddings(model=\"nomic-embed-text:v1.5\")\n",
    "# vectorstore_nomic = Chroma.from_documents(\n",
    "#     documents=chunks[:10],  # Use first 10 chunks for quick test\n",
    "#     embedding=embeddings_nomic,\n",
    "#     collection_name=\"test_nomic\"\n",
    "# )\n",
    "# retriever_nomic = vectorstore_nomic.as_retriever(search_kwargs={\"k\": 2})\n",
    "# docs_nomic = retriever_nomic.invoke(test_query)\n",
    "\n",
    "# print(f\"\\nTop retrieved document:\")\n",
    "# print(f\"{docs_nomic[0].page_content[:200]}...\\n\")\n",
    "\n",
    "# # Test with embeddinggemma\n",
    "# print(\"=\" * 80)\n",
    "# print(\"Using embeddinggemma:\")\n",
    "# print(\"=\" * 80)\n",
    "# embeddings_gemma = OllamaEmbeddings(model=\"embeddinggemma:latest\")\n",
    "# vectorstore_gemma = Chroma.from_documents(\n",
    "#     documents=chunks[:10],\n",
    "#     embedding=embeddings_gemma,\n",
    "#     collection_name=\"test_gemma\"\n",
    "# )\n",
    "# retriever_gemma = vectorstore_gemma.as_retriever(search_kwargs={\"k\": 2})\n",
    "# docs_gemma = retriever_gemma.invoke(test_query)\n",
    "\n",
    "# print(f\"\\nTop retrieved document:\")\n",
    "# print(f\"{docs_gemma[0].page_content[:200]}...\\n\")\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"\\n‚ÑπÔ∏è  Both models perform well. Choose based on your preference!\")\n",
    "# print(\"   - nomic-embed-text: Smaller (274MB), general-purpose\")\n",
    "# print(\"   - embeddinggemma: Larger (621MB), optimized for Gemma models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
